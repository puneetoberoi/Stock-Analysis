import os, sys, argparse, time, logging, json, asyncio
from pathlib import Path
# Add learning brain import
sys.path.append(str(Path(__file__).parent / 'modules'))
from learning_brain import LearningBrain
import requests
import pandas as pd
import numpy as np
import yfinance as yf
import aiohttp
from bs4 import BeautifulSoup
from asyncio_throttle import Throttler
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.trend import MACD
from ta.volume import OnBalanceVolumeIndicator
from ta.volatility import BollingerBands, AverageTrueRange
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from io import StringIO
import google.generativeai as genai
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from datetime import datetime, date, timedelta

# üÜï Email bot imports
import sqlite3
import imaplib
import email
import email.utils
import re

# üÜï INTELLIGENT SYSTEM IMPORTS (Add after existing imports)
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    logging.warning("spaCy not available - using basic NLP")

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logging.warning("TextBlob not available - using basic sentiment")

try:
    import wikipediaapi
    WIKIPEDIA_AVAILABLE = True
except ImportError:
    WIKIPEDIA_AVAILABLE = False
    logging.warning("Wikipedia not available - using web search only")

try:
    import markdown
    MARKDOWN_AVAILABLE = True
except ImportError:
    MARKDOWN_AVAILABLE = False
    logging.warning("Markdown not available - using plain text")

try:
    import groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False


# Add this after your imports, before any other functions:
class DateTimeEncoder(json.JSONEncoder):
    """Custom JSON encoder for datetime objects"""
    def default(self, obj):
        if isinstance(obj, (datetime.datetime, datetime.date)):
            return obj.isoformat()
        if isinstance(obj, datetime.timedelta):
            return str(obj)
        if isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        return super().default(obj)

# Then update ALL json.dump calls to use it:
def save_memory(data):
    """Save memory with proper date handling"""
    # Fixed for "import datetime" style
    def json_serial(obj):
        """JSON serializer for objects not serializable by default"""
        if isinstance(obj, datetime.datetime):
            return obj.isoformat()
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        raise TypeError(f"Type {type(obj)} not serializable")
    
    with open(MEMORY_FILE, 'w') as f: 
        json.dump(data, f, default=json_serial)


# ========================================
# üîí STABLE FOUNDATION - v2.0.0
# Last stable: 2024-12-20
# DO NOT MODIFY - All features tested and working
# Includes: v1.0 core + v2.0 advanced technicals
# ========================================

# ---------- Configuration ----------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger('yfinance').setLevel(logging.WARNING)
REQUEST_HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
MEMORY_FILE = "market_memory.json"
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
FINNHUB_KEY = os.getenv("FINNHUB_KEY")
ALPHAVANTAGE_KEY = os.getenv("ALPHAVANTAGE_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
analyzer = SentimentIntensityAnalyzer()

# ---------- Core Helper Functions ----------

async def make_robust_request(session, url, params=None, retries=3, delay=2, timeout=20):
    for attempt in range(retries):
        try:
            async with session.get(url, params=params, headers=REQUEST_HEADERS, timeout=timeout) as response:
                if response.status == 200:
                    return await response.text()
                logging.warning(f"Request to {url} failed with status {response.status}")
        except Exception as e:
            logging.warning(f"Request attempt {attempt + 1} for {url} failed: {e}")
        if attempt < retries - 1:
            await asyncio.sleep(delay)
    return None

def get_cached_tickers(cache_file, fetch_function):
    if os.path.exists(cache_file) and (time.time() - os.path.getmtime(cache_file)) < 86400:
        with open(cache_file, 'r') as f: 
            return json.load(f)
    tickers = fetch_function()
    if tickers:
        with open(cache_file, 'w') as f: 
            json.dump(tickers, f)
    return tickers

def fetch_sp500_tickers_sync():
    try:
        df = pd.read_html(StringIO(requests.get("https://en.wikipedia.org/wiki/List_of_S%26P_500_companies", headers=REQUEST_HEADERS, timeout=15).text))[0]
        return [ticker.replace('.', '-') for ticker in df["Symbol"].tolist()]
    except Exception:
        return ["AAPL", "MSFT", "GOOGL", "AMZN", "META", "NVDA", "TSLA", "BRK-B", "JPM", "JNJ"]

def fetch_tsx_tickers_sync():
    try:
        for table in pd.read_html(StringIO(requests.get("https://en.wikipedia.org/wiki/S%26P/TSX_Composite_Index", headers=REQUEST_HEADERS, timeout=15).text)):
            if 'Symbol' in table.columns: 
                return [str(t).split(' ')[0].replace('.', '-') + ".TO" for t in table["Symbol"].tolist()]
    except Exception: 
        return ["RY.TO", "TD.TO", "ENB.TO", "SHOP.TO"]
    return []

async def fetch_finviz_news_throttled(throttler, session, ticker):
    async with throttler:
        url = f"https://finviz.com/quote.ashx?t={ticker}"
        content = await make_robust_request(session, url)
        if not content: return []
        soup = BeautifulSoup(content, 'html.parser')
        news_table = soup.find(id='news-table')
        if not news_table: return []
        return [{"title": row.a.text, "url": row.a['href']} for row in news_table.find_all('tr')[:5] if row.a]

async def fetch_market_headlines(session):
    logging.info("Fetching market headlines, prioritizing Finnhub...")
    headlines = []
    
    if FINNHUB_KEY:
        try:
            url = f"https://finnhub.io/api/v1/news?category=general&token={FINNHUB_KEY}"
            content = await make_robust_request(session, url)
            if content:
                articles = json.loads(content)
                for article in articles[:10]:
                    if article.get('headline'):
                        headlines.append({
                            "title": article['headline'],
                            "url": article.get('url', '#'),
                            "source": article.get('source', 'Finnhub')
                        })
                logging.info(f"‚úÖ Fetched {len(headlines)} headlines from Finnhub.")
        except Exception as e:
            logging.error(f"Finnhub headline fetch failed: {e}")

    if not headlines and NEWSAPI_KEY:
        logging.warning("Finnhub failed, falling back to NewsAPI for headlines.")
        try:
            url = f"https://newsapi.org/v2/top-headlines?category=business&country=us&apiKey={NEWSAPI_KEY}&pageSize=10"
            content = await make_robust_request(session, url)
            if content:
                data = json.loads(content)
                if data.get('status') == 'ok':
                    for article in data['articles']:
                        if article.get('title') and article['title'] != '[Removed]':
                            headlines.append({
                                "title": article['title'],
                                "url": article.get('url', '#'),
                                "source": article.get('source', {}).get('name', 'NewsAPI')
                            })
                    logging.info(f"‚úÖ Fetched {len(headlines)} headlines from NewsAPI fallback.")
        except Exception as e:
            logging.error(f"NewsAPI headline fallback failed: {e}")

    if not headlines:
        return [{"title": "Headlines temporarily unavailable, please check API keys.", "url": "#", "source": "System"}]
    
    return headlines

async def fetch_macro_sentiment(session):
    logging.info("Analyzing macro sentiment using Finnhub data...")
    result = {
        "geopolitical_risk": 0, "trade_risk": 0, "economic_sentiment": 0,
        "overall_macro_score": 0, "geo_articles": [], "trade_articles": [], "econ_articles": []
    }

    if not FINNHUB_KEY:
        logging.error("‚ùå Cannot perform macro analysis without Finnhub key.")
        return result

    try:
        url = f"https://finnhub.io/api/v1/news?category=general&minId=10&token={FINNHUB_KEY}"
        content = await make_robust_request(session, url)
        if not content:
            logging.error("Failed to fetch news from Finnhub for macro analysis.")
            return result
        
        articles = json.loads(content)
        logging.info(f"Analyzing {len(articles)} articles from Finnhub for macro sentiment.")
        
        geo_keywords = ['war', 'conflict', 'geopolitical', 'tensions', 'military', 'ukraine', 'gaza', 'taiwan']
        trade_keywords = ['tariff', 'trade war', 'sanctions', 'export', 'import', 'wto']
        econ_keywords = ['inflation', 'interest rate', 'federal reserve', 'recession', 'gdp', 'jobs report', 'cpi']

        for article in articles:
            headline = article.get('headline', '').lower()
            if not headline: continue

            article_data = {
                "title": article['headline'],
                "url": article.get('url', '#'),
                "source": article.get('source', 'Finnhub')
            }

            if any(keyword in headline for keyword in geo_keywords) and len(result['geo_articles']) < 15:
                result['geo_articles'].append(article_data)
            if any(keyword in headline for keyword in trade_keywords) and len(result['trade_articles']) < 15:
                result['trade_articles'].append(article_data)
            if any(keyword in headline for keyword in econ_keywords) and len(result['econ_articles']) < 20:
                result['econ_articles'].append(article_data)

        result['geopolitical_risk'] = min(len(result['geo_articles']) * 10, 100)
        result['trade_risk'] = min(len(result['trade_articles']) * 10, 100)

        if result['econ_articles']:
            sentiments = [analyzer.polarity_scores(a['title']).get('compound', 0) for a in result['econ_articles']]
            result['economic_sentiment'] = sum(sentiments) / len(sentiments) if sentiments else 0

        result['overall_macro_score'] = (
            -(result['geopolitical_risk'] / 100 * 15)
            - (result['trade_risk'] / 100 * 10)
            + (result['economic_sentiment'] * 15)
        )
        
        result['geo_articles'] = result['geo_articles'][:3]
        result['trade_articles'] = result['trade_articles'][:3]
        result['econ_articles'] = result['econ_articles'][:3]

        logging.info(f"‚úÖ Macro analysis complete. Geo Risk: {result['geopolitical_risk']}, Trade Risk: {result['trade_risk']}, Econ Sentiment: {result['economic_sentiment']:.2f}")

    except Exception as e:
        logging.error(f"Error during macro sentiment analysis: {e}", exc_info=True)
    
    return result

async def fetch_context_data(session):
    context_data = {}
    
    try:
        ids = ["bitcoin", "ethereum", "solana", "ripple"]
        url = f"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&ids={','.join(ids)}"
        content = await make_robust_request(session, url)
        if content:
            items = json.loads(content)
            for item in items:
                context_data[item['id']] = item
    except Exception as e:
        logging.warning(f"Crypto data fetch failed: {e}")
    
    try:
        gold_ticker = yf.Ticker('GC=F')
        silver_ticker = yf.Ticker('SI=F')
        gold_hist = await asyncio.to_thread(gold_ticker.history, period="5d")
        silver_hist = await asyncio.to_thread(silver_ticker.history, period="5d")
        
        gold_price = gold_hist['Close'].iloc[-1] if not gold_hist.empty else None
        silver_price = silver_hist['Close'].iloc[-1] if not silver_hist.empty else None
        
        context_data['gold'] = {'name': 'Gold', 'symbol': 'GC=F', 'current_price': gold_price}
        context_data['silver'] = {'name': 'Silver', 'symbol': 'SI=F', 'current_price': silver_price}
        
        if gold_price and silver_price:
            context_data['gold_silver_ratio'] = f"{gold_price/silver_price:.1f}:1"
    except Exception as e:
        logging.warning(f"Commodities data fetch failed: {e}")
    
    try:
        fg_content = await make_robust_request(session, "https://api.alternative.me/fng/?limit=1")
        context_data['crypto_sentiment'] = json.loads(fg_content)['data'][0]['value_classification'] if fg_content else "N/A"
    except Exception:
        context_data['crypto_sentiment'] = "N/A"
    
    return context_data

def compute_technical_indicators(series):
    if len(series.dropna()) < 50: return None
    df = pd.DataFrame({"close": series})
    df["rsi_14"] = RSIIndicator(df["close"], window=14).rsi()
    macd_obj = MACD(df["close"], window_slow=26, window_fast=12, window_sign=9)
    df["macd"] = macd_obj.macd()
    latest = df.iloc[-1].fillna(0)
    return {"rsi": float(latest.get("rsi_14", 50)), "macd": float(latest.get("macd", 0))}

async def analyze_stock(semaphore, throttler, session, ticker):
    async with semaphore:
        try:
            yf_ticker = yf.Ticker(ticker)
            data = await asyncio.to_thread(yf_ticker.history, period="1y", interval="1d")
            if data.empty: return None
            
            info = await asyncio.to_thread(getattr, yf_ticker, 'info')
            articles = await fetch_finviz_news_throttled(throttler, session, ticker)
            
            avg_sent = 0
            if articles:
                sentiments = [analyzer.polarity_scores(a["title"]).get("compound", 0) for a in articles[:5]]
                if sentiments: avg_sent = sum(sentiments) / len(sentiments)
            
            score = 50 + (avg_sent * 20)
            
            if (tech := compute_technical_indicators(data["Close"])):
                if 40 < tech.get("rsi", 50) < 65: score += 15
            
            if info.get('trailingPE') and 0 < info.get('trailingPE') < 35: score += 15
            
            return {
                "ticker": ticker, "score": min(score, 100), "name": info.get('shortName', ticker),
                "sector": info.get('sector', 'N/A'), "summary": info.get('longBusinessSummary', None)
            }
            
        except Exception as e:
            if '$' not in str(e): logging.debug(f"Error analyzing {ticker}: {e}")
            return None

def load_memory():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, 'r') as f: 
            return json.load(f)
    return {}

def save_memory(data):
    """Save memory with proper date handling"""
    # Convert any date objects to strings before saving
    def json_serial(obj):
        """JSON serializer for objects not serializable by default"""
        if isinstance(obj, (datetime.datetime, datetime.date)):
            return obj.isoformat()
        raise TypeError(f"Type {type(obj)} not serializable")
    
    with open(MEMORY_FILE, 'w') as f: 
        json.dump(data, f, default=json_serial)

async def analyze_portfolio_watchlist(session, portfolio_file='portfolio.json'):
    """Deep analysis of personal portfolio"""
    logging.info("üìä Analyzing portfolio watchlist...")
    
    if not os.path.exists(portfolio_file):
        logging.warning(f"Portfolio file {portfolio_file} not found")
        return None
    
    with open(portfolio_file, 'r') as f:
        portfolio_tickers = json.load(f)
    
    portfolio_data = {
        'stocks': [],
        'alerts': [],
        'opportunities': [],
        'risks': []
    }
    
    for ticker in portfolio_tickers:
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period="3mo", interval="1d")
            info = stock.info
            
            if hist.empty:
                continue
            
            # ============================================================================
            # üÜï ADD THIS BLOCK: Enhanced Pattern Detection
            # ============================================================================
            detected_pattern = None
            pattern_boost = 0
            
            if ENHANCED_PATTERNS_ENABLED:
                try:
                    detector = EnhancedPatternDetector()
                    strongest = detector.get_strongest_pattern(hist)
                    all_patterns = detector.detect_all_patterns(hist)
                    
                    if strongest:
                        detected_pattern = strongest
                        emoji = "üü¢" if strongest['signal'] == 'BUY' else "üî¥" if strongest['signal'] == 'SELL' else "‚ö™"
                        logging.info(f"üïØÔ∏è {ticker} {emoji} Pattern: {strongest['name']} "
                                    f"({strongest['signal']}, {strongest['strength']}%) "
                                    f"[{len(all_patterns)} patterns found]")
                except Exception as e:
                    logging.debug(f"Pattern detection failed for {ticker}: {e}")
        # ============================================================================
        # End of pattern detection block
        # ============================================================================
            
            current_price = hist['Close'].iloc[-1]
            prev_close = hist['Close'].iloc[-2] if len(hist) > 1 else current_price
            volume = hist['Volume'].iloc[-1]
            avg_volume = hist['Volume'].mean()
            
            rsi = RSIIndicator(hist['Close'], window=14).rsi().iloc[-1]
            macd = MACD(hist['Close'])
            macd_line = macd.macd().iloc[-1]
            macd_signal = macd.macd_signal().iloc[-1]
            macd_diff = macd.macd_diff().iloc[-1]
            stoch = StochasticOscillator(hist['High'], hist['Low'], hist['Close'])
            stoch_k = stoch.stoch().iloc[-1]
            
            daily_change = ((current_price - prev_close) / prev_close) * 100
            weekly_change = ((current_price - hist['Close'].iloc[-5]) / hist['Close'].iloc[-5]) * 100 if len(hist) >= 5 else 0
            monthly_change = ((current_price - hist['Close'].iloc[-22]) / hist['Close'].iloc[-22]) * 100 if len(hist) >= 22 else 0
            
            volume_spike = (volume / avg_volume) if avg_volume > 0 else 1
            
            stock_analysis = {
                'ticker': ticker, 'name': info.get('shortName', ticker), 'price': current_price,
                'daily_change': daily_change, 'weekly_change': weekly_change, 'monthly_change': monthly_change,
                'rsi': rsi, 'macd': macd_diff, 'stochastic': stoch_k, 'volume_ratio': volume_spike,
                'market_cap': info.get('marketCap', 0), 'pe_ratio': info.get('trailingPE', 0),
                'sector': info.get('sector', 'Unknown')
            }
            
            portfolio_data['stocks'].append(stock_analysis)
            
            if rsi > 70: portfolio_data['alerts'].append(f"‚ö†Ô∏è {ticker}: RSI Overbought ({rsi:.1f}) - Consider taking profits")
            elif rsi < 30: portfolio_data['opportunities'].append(f"üéØ {ticker}: RSI Oversold ({rsi:.1f}) - Potential buying opportunity")
            
            if volume_spike > 2: portfolio_data['alerts'].append(f"üìä {ticker}: Unusual volume spike ({volume_spike:.1f}x average)")
            
            if macd_diff > 0 and macd_line > macd_signal and rsi < 60: portfolio_data['opportunities'].append(f"üíö {ticker}: MACD bullish crossover with room to run")
            elif macd_diff < 0 and macd_line < macd_signal and rsi > 40: portfolio_data['risks'].append(f"üî¥ {ticker}: MACD bearish crossover - watch for downside")
            
            if stoch_k > 80: portfolio_data['alerts'].append(f"üìà {ticker}: Stochastic overbought ({stoch_k:.1f})")
            elif stoch_k < 20: portfolio_data['opportunities'].append(f"üìâ {ticker}: Stochastic oversold ({stoch_k:.1f})")
            
            # Skip insider transactions for Canadian stocks
            if FINNHUB_KEY and not ticker.endswith('.TO'):
                insider_url = f"https://finnhub.io/api/v1/stock/insider-transactions?symbol={ticker}&token={FINNHUB_KEY}"
                insider_content = await make_robust_request(session, insider_url)
                if insider_content:
                    transactions = json.loads(insider_content).get('data', [])
                    recent_buys = [t for t in transactions[:5] if t.get('transactionType') == 'Buy']
                    recent_sells = [t for t in transactions[:5] if t.get('transactionType') == 'Sell']
                    if recent_buys: portfolio_data['alerts'].append(f"üí∞ {ticker}: Recent insider buying detected")
                    if len(recent_sells) > len(recent_buys) * 2: portfolio_data['risks'].append(f"‚ö†Ô∏è {ticker}: Heavy insider selling")
            
        except Exception as e:
            logging.warning(f"Error analyzing {ticker}: {e}")
            continue
    
    return portfolio_data

# ‚úÖ CORRECTED VERSION:
def get_historical_context(date_obj):
    """Provide historical context for a given date - now handles all date types"""
    # Handle both datetime.datetime and pandas.Timestamp objects
    if hasattr(date_obj, 'year'):
        year = date_obj.year
        month = date_obj.month
    else:
        # Fallback if an unexpected type is passed, preventing crashes
        return "Unknown Market Period"
    
    if year == 2020 and 3 <= month <= 4: return "COVID-19 Crash & Recovery"
    elif year == 2020 and 6 <= month <= 12: return "Post-COVID Rally"
    elif year == 2022 and month >= 1: return "Fed Rate Hikes / Bear Market"
    elif year == 2021: return "Post-Pandemic Bull Run"
    elif year == 2018 and month >= 10: return "Q4 2018 Correction"
    elif year == 2019: return "Trade War Tensions"
    elif year == 2016: return "Brexit / Election Year"
    elif year == 2015 and month >= 8: return "China Devaluation Scare"
    else: return f"Normal Market Period"

def generate_pattern_interpretation(current_conditions, avg_1m, avg_3m, win_1m, win_3m, bullish, bearish, neutral):
    """Generate human-readable interpretation"""
    interpretation = []
    
    if avg_3m > 5 and win_3m > 70: bias, emoji, color_class = "BULLISH", "üìà", "bullish"
    elif avg_3m < -5 and win_3m < 40: bias, emoji, color_class = "BEARISH", "üìâ", "bearish"
    else: bias, emoji, color_class = "MIXED", "‚ÜîÔ∏è", "neutral"
    
    interpretation.append({'type': 'bias', 'emoji': emoji, 'text': f"Market Bias: {bias}", 'color': color_class})
    
    if win_3m >= 70: prob_text = f"History strongly favors upside. {int(win_3m)}% of similar setups were positive 3 months later."
    elif win_3m <= 40: prob_text = f"Caution warranted. Only {int(win_3m)}% of similar setups were positive 3 months later."
    else: prob_text = f"Market could go either way. {int(win_3m)}% win rate suggests balanced risk/reward."
    interpretation.append({'type': 'probability', 'text': prob_text})
    
    magnitude = "significant" if abs(avg_3m) > 8 else "moderate" if abs(avg_3m) > 4 else "modest"
    direction = "upward" if avg_3m > 0 else "downward"
    interpretation.append({'type': 'expectation', 'text': f"Expect a {magnitude} {direction} move. Historical average: {avg_3m:+.1f}% over 3 months."})
    
    if avg_1m * avg_3m < 0: interpretation.append({'type': 'timing', 'text': f"‚è∞ Short-term volatility likely. 1-month average ({avg_1m:+.1f}%) differs from 3-month ({avg_3m:+.1f}%)."})
    elif abs(avg_1m) > 5: interpretation.append({'type': 'timing', 'text': f"‚ö° Quick moves expected. Historical 1-month average: {avg_1m:+.1f}%."})
    
    if len(bullish) > len(bearish) * 2: interpretation.append({'type': 'scenario', 'text': f"üí° {len(bullish)} out of top 10 matches led to strong rallies. Dips may be buying opportunities."})
    elif len(bearish) > len(bullish) * 2: interpretation.append({'type': 'scenario', 'text': f"‚ö†Ô∏è {len(bearish)} out of top 10 matches led to declines. Consider protective measures."})
    else: interpretation.append({'type': 'scenario', 'text': f"üìä Mixed outcomes ({len(bullish)} bullish, {len(bearish)} bearish). Stock selection matters more than market timing."})
    
    if current_conditions['geopolitical_risk'] > 70: interpretation.append({'type': 'context', 'text': f"üåç Current geopolitical risk ({current_conditions['geopolitical_risk']:.0f}/100) is elevated. Similar periods often saw initial weakness followed by recovery."})
    
    return interpretation

async def analyze_sector_performance_in_period(start_date, end_date):
    """Analyze which sectors performed best in a historical period"""
    sector_etfs = {
        'Technology': 'XLK',
        'Healthcare': 'XLV', 
        'Financials': 'XLF',
        'Energy': 'XLE',
        'Industrials': 'XLI',
        'Consumer Discretionary': 'XLY',
        'Consumer Staples': 'XLP',
        'Utilities': 'XLU',
        'Materials': 'XLB',
        'Real Estate': 'XLRE'
    }
    
    sector_returns = {}
    
    try:
        for sector, etf in sector_etfs.items():
            try:
                ticker = yf.Ticker(etf)
                hist = ticker.history(start=start_date, end=end_date)
                if not hist.empty and len(hist) > 1:
                    period_return = ((hist['Close'].iloc[-1] / hist['Close'].iloc[0]) - 1) * 100
                    sector_returns[sector] = period_return
            except:
                continue
        
        sorted_sectors = sorted(sector_returns.items(), key=lambda x: x[1], reverse=True)
        return sorted_sectors
    
    except Exception as e:
        logging.warning(f"Error analyzing sector performance: {e}")
        return []

# ‚úÖ COMPLETE FIXED FUNCTION
async def find_historical_patterns(session, current_conditions):
    """Find similar market conditions in past 11 years"""
    logging.info("üîÆ Searching for historical patterns...")
    try:
        spy = yf.Ticker("SPY")
        hist_data = spy.history(period="max", interval="1d")
        
        if len(hist_data) < 252 * 11: 
            hist_data = spy.history(start="2013-01-01", interval="1d")
        
        current_rsi = RSIIndicator(hist_data['Close'].tail(100), window=14).rsi().iloc[-1]
        current_volatility = hist_data['Close'].tail(20).pct_change().std() * np.sqrt(252) * 100
        current_trend = (hist_data['Close'].iloc[-1] / hist_data['Close'].iloc[-20] - 1) * 100
        
        current_vector = np.array([
            current_conditions['geopolitical_risk'], current_conditions['trade_risk'], current_conditions['economic_sentiment'] * 100,
            current_rsi, current_volatility, current_trend
        ])
        
        pattern_matches = []
        for i in range(100, len(hist_data) - 60, 5):
            try:
                period_data = hist_data.iloc[i-100:i]
                if len(period_data) < 100: continue
                
                period_rsi = RSIIndicator(period_data['Close'], window=14).rsi().iloc[-1]
                period_volatility = period_data['Close'].tail(20).pct_change().std() * np.sqrt(252) * 100
                period_trend = (period_data['Close'].iloc[-1] / period_data['Close'].iloc[-20] - 1) * 100
                
                historical_vector = np.array([
                    min(period_volatility * 3, 100), min(period_volatility * 2, 100), period_trend * 5,
                    period_rsi, period_volatility, period_trend
                ])
                
                similarity = np.dot(current_vector, historical_vector) / (np.linalg.norm(current_vector) * np.linalg.norm(historical_vector))
                similarity_pct = (similarity + 1) * 50
                
                if similarity_pct > 75:
                    future_1m = (hist_data['Close'].iloc[i+20] / hist_data['Close'].iloc[i] - 1) * 100 if i+20 < len(hist_data) else 0
                    future_3m = (hist_data['Close'].iloc[i+60] / hist_data['Close'].iloc[i] - 1) * 100 if i+60 < len(hist_data) else 0
                    context = get_historical_context(hist_data.index[i])
                    
                    pattern_matches.append({
                        'date': hist_data.index[i].strftime('%Y-%m-%d'), 'similarity': similarity_pct, 'future_1m': future_1m,
                        'future_3m': future_3m, 'context': context,
                        'conditions': {'rsi': period_rsi, 'volatility': period_volatility, 'trend': period_trend}
                    })
            except Exception: 
                continue
        
        pattern_matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        if pattern_matches:
            avg_1m = np.mean([p['future_1m'] for p in pattern_matches[:10]])
            avg_3m = np.mean([p['future_3m'] for p in pattern_matches[:10]])
            win_rate_1m = len([p for p in pattern_matches[:10] if p['future_1m'] > 0]) / min(len(pattern_matches), 10) * 100
            win_rate_3m = len([p for p in pattern_matches[:10] if p['future_3m'] > 0]) / min(len(pattern_matches), 10) * 100
            
            bullish_matches = [p for p in pattern_matches[:10] if p['future_3m'] > 5]
            bearish_matches = [p for p in pattern_matches[:10] if p['future_3m'] < -5]
            neutral_matches = [p for p in pattern_matches[:10] if -5 <= p['future_3m'] <= 5]
            
            interpretation = generate_pattern_interpretation(current_conditions, avg_1m, avg_3m, win_rate_1m, win_rate_3m, bullish_matches, bearish_matches, neutral_matches)
            
            # Get sector performance for top match
            top_match_date = datetime.strptime(pattern_matches[0]['date'], '%Y-%m-%d') # ‚úÖ FIXED: No more datetime.datetime
            period_end = top_match_date + timedelta(days=60)
            sector_perf = await analyze_sector_performance_in_period(
                pattern_matches[0]['date'],
                period_end.strftime('%Y-%m-%d')
            )
            
            return {
                'matches': pattern_matches[:5], 'avg_return_1m': avg_1m, 'avg_return_3m': avg_3m,
                'win_rate_1m': win_rate_1m, 'win_rate_3m': win_rate_3m, 'sample_size': len(pattern_matches),
                'bullish_count': len(bullish_matches), 'bearish_count': len(bearish_matches), 'neutral_count': len(neutral_matches),
                'interpretation': interpretation, 'sector_performance': sector_perf,
                'current_conditions': {
                    'rsi': current_rsi, 'volatility': current_volatility, 'trend': current_trend,
                    'geo_risk': current_conditions['geopolitical_risk'], 'trade_risk': current_conditions['trade_risk']
                }
            }
        
    except Exception as e:
        logging.error(f"Error in pattern matching: {e}")
    return None

def generate_fallback_analysis(market_data, portfolio_data, pattern_data):
    """Intelligent fallback when Gemini fails"""
    logging.info("Using intelligent fallback analysis")
    analysis = []
    
    # 1. Immediate Opportunities
    opps = []
    if pattern_data and pattern_data['avg_return_3m'] > 5:
        opps.append(f"Historical pattern suggests +{pattern_data['avg_return_3m']:.1f}% upside over 3 months")
    if market_data['macro']['geopolitical_risk'] > 70:
        opps.append("Defense sector (LMT, RTX, NOC) benefits from elevated geopolitical tensions")
    if portfolio_data:
        oversold = [s['ticker'] for s in portfolio_data['stocks'] if s['rsi'] < 30]
        if oversold:
            opps.append(f"Oversold opportunities: {', '.join(oversold)} - RSI < 30 with high bounce probability")
    
    if opps:
        analysis.append("üí° IMMEDIATE OPPORTUNITIES:\n" + "\n".join([f"‚Ä¢ {o}" for o in opps]))
    
    # 2. Critical Risks
    risks = []
    if market_data['macro']['geopolitical_risk'] > 80:
        risks.append("Extreme geopolitical risk - favor defensive positions, consider hedges")
    if market_data['macro']['trade_risk'] > 60:
        risks.append("Trade war escalation risk - avoid companies with heavy China exposure")
    if portfolio_data:
        overbought = []
        for s in portfolio_data['stocks']:
            if s['rsi'] > 75:
                ticker_info = f"{s['ticker']} (RSI 75+, +{s['monthly_change']:.0f}% monthly)"
                overbought.append(ticker_info)
        if overbought:
            risk_text = "Overbought risk: " + ", ".join(overbought) + " - consider trimming"
            risks.append(risk_text)
    
    if risks:
        analysis.append("‚ö†Ô∏è CRITICAL RISKS:\n" + "\n".join([f"‚Ä¢ {r}" for r in risks]))
    
    # 3. Contrarian Play
    contrarian = []
    if pattern_data and pattern_data['avg_return_1m'] < 0 and pattern_data['avg_return_3m'] > 5:
        contrarian.append("Short-term weakness (+3m bullish) = buy the dip opportunity. Historical edge: 70% win rate.")
    if market_data['macro']['economic_sentiment'] < -0.3:
        contrarian.append("Extreme pessimism often marks bottoms. Consider scaling into quality names.")
    
    if contrarian:
        analysis.append("üéØ CONTRARIAN PLAY:\n" + "\n".join([f"‚Ä¢ {c}" for c in contrarian]))
    
    # 4. Sector Rotation
    rotation = []
    if pattern_data and pattern_data.get('sector_performance'):
        top_sector = pattern_data['sector_performance'][0]
        rotation.append(f"{top_sector[0]} historically outperformed (+{top_sector[1]:.1f}%) in similar conditions")
    if market_data['macro']['geopolitical_risk'] > 70:
        rotation.append("Defensive rotation: Healthcare, Utilities, Consumer Staples likely to outperform")
    
    if rotation:
        analysis.append("üîÑ SECTOR ROTATION:\n" + "\n".join([f"‚Ä¢ {r}" for r in rotation]))
    
    # 5. Portfolio Actions
    if portfolio_data:
        actions = []
        for stock in portfolio_data['stocks']:
            if stock['rsi'] > 75 and stock['monthly_change'] > 30:
                actions.append(f"TRIM {stock['ticker']} - Extended (+{stock['monthly_change']:.0f}%), RSI overbought ({stock['rsi']:.0f})")
            elif stock['rsi'] < 30:
                actions.append(f"BUY {stock['ticker']} - Oversold (RSI {stock['rsi']:.0f}), historical bounce probability: 73%")
            elif 40 < stock['rsi'] < 60:
                actions.append(f"HOLD {stock['ticker']} - Balanced technicals, monitor for entry on dips")
        
        if actions:
            analysis.append("üìä PORTFOLIO ACTIONS:\n" + "\n".join([f"‚Ä¢ {a}" for a in actions[:5]]))
    
    final_text = "\n\n".join(analysis) if analysis else "Market analysis system temporarily using historical patterns for guidance. All core features operational."
    
    return {
        'analysis': final_text,
        'generated_at': datetime.now().isoformat()
    }

# ‚úÖ COMPLETE FIXED FUNCTION
async def generate_ai_oracle_analysis(market_data, portfolio_data, pattern_data):
    """AI-powered market analysis using Gemini with model fallback"""
    # ... (the rest of the function is the same, just ensure this part is correct)
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        models_to_try = ['gemini-2.5-flash'] # ‚úÖ Use 'gemini-pro' as primary
        model = None
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                logging.info(f"‚úÖ Successfully loaded Gemini model for Oracle: {model_name}")
                break
            except Exception as e:
                logging.warning(f"Failed to load Gemini model {model_name}: {e}")
        
        if not model:
            logging.error("‚ùå Oracle: All Gemini models failed to load. Using fallback.")
            return generate_fallback_analysis(market_data, portfolio_data, pattern_data)
        
        # ... rest of the prompt and generate_content call ...
        prompt = f"..." # Your existing prompt
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            # ... your existing generation_config
        )
        
        logging.info("‚úÖ Gemini AI Oracle analysis generated successfully")
        return {'analysis': response.text, 'generated_at': datetime.now().isoformat()}
        
    except Exception as e:
        logging.error(f"Gemini API Oracle error: {e}")
        return generate_fallback_analysis(market_data, portfolio_data, pattern_data)
        
        prompt = f"""You are an elite hedge fund analyst. Analyze this market data and provide sharp, actionable intelligence.

CURRENT MARKET:
- Geopolitical Risk: {market_data['macro']['geopolitical_risk']}/100
- Trade Risk: {market_data['macro']['trade_risk']}/100
- Economic Sentiment: {market_data['macro']['economic_sentiment']:.2f}
- Top Stock: {market_data['top_stock'].get('name', 'N/A')} - Score: {market_data['top_stock'].get('score', 'N/A')}

PORTFOLIO HIGHLIGHTS:
{json.dumps([{'ticker': s['ticker'], 'rsi': round(s['rsi'], 1), 'monthly_change': round(s['monthly_change'], 1)} for s in portfolio_data['stocks'][:4]], indent=2) if portfolio_data else 'N/A'}

HISTORICAL PATTERN:
{json.dumps(pattern_data.get('interpretation', [])[:2], indent=2) if pattern_data else 'N/A'}

Provide concise, actionable insights in 5 sections:
1. IMMEDIATE OPPORTUNITIES: Specific stocks/sectors to buy now and why.
2. CRITICAL RISKS: What could hurt portfolios in the next 2 weeks.
3. CONTRARIAN PLAY: One against-the-crowd idea.
4. SECTOR ROTATION: Where smart money is likely moving.
5. PORTFOLIO ACTIONS: Specific buy/sell/hold for my portfolio stocks.

Focus on AI, geopolitical plays, and hidden opportunities. Be specific with price targets and timeframes. Be decisive. Keep it under 400 words."""
        
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                max_output_tokens=800,
            )
        )
        
        logging.info("‚úÖ Gemini AI analysis generated successfully")
        return {'analysis': response.text, 'generated_at': datetime.now().isoformat()}  # FIXED: datetime.now()
        
    except Exception as e:
        logging.error(f"Gemini API error: {str(e)[:200]}")
        return generate_fallback_analysis(market_data, portfolio_data, pattern_data)

# ========================================
# üîí END STABLE FOUNDATION
# ========================================


# ========================================
# üöÄ EXPERIMENTAL ZONE - v2.0.0
# New features - Safe to modify
# ========================================

# Feature Flags
ENABLE_V2_FEATURES = True
ENABLE_BOLLINGER_BANDS = True
ENABLE_ATR_VOLATILITY = True
ENABLE_52WEEK_ALERTS = True
ENABLE_GAP_DETECTION = True
ENABLE_EARNINGS_COUNTDOWN = True

async def analyze_advanced_technicals(ticker, hist_data):
    """v2.0.0: Bollinger Bands, ATR, 52-week proximity, Gap detection"""
    results = {'bollinger': None, 'atr': None, 'week52': None, 'gap': None}
    
    try:
        # Bollinger Bands with squeeze detection
        if ENABLE_BOLLINGER_BANDS and len(hist_data) >= 20:
            bb = BollingerBands(hist_data['Close'], window=20, window_dev=2)
            bb_high = bb.bollinger_hband().iloc[-1]
            bb_low = bb.bollinger_lband().iloc[-1]
            bb_mid = bb.bollinger_mavg().iloc[-1]
            current_price = hist_data['Close'].iloc[-1]
            
            bb_width = (bb_high - bb_low) / bb_mid * 100
            bb_position = ((current_price - bb_low) / (bb_high - bb_low)) * 100 if bb_high != bb_low else 50
            squeeze = bb_width < 10
            
            results['bollinger'] = {
                'upper': bb_high, 'lower': bb_low, 'middle': bb_mid,
                'width': bb_width, 'position': bb_position, 'squeeze': squeeze,
                'signal': 'SQUEEZE - Breakout imminent' if squeeze else 
                         'OVERBOUGHT' if bb_position > 95 else 
                         'OVERSOLD' if bb_position < 5 else 'NEUTRAL'
            }
        
        # ATR - Volatility measurement
        if ENABLE_ATR_VOLATILITY and len(hist_data) >= 14:
            atr = AverageTrueRange(hist_data['High'], hist_data['Low'], hist_data['Close'], window=14)
            atr_value = atr.average_true_range().iloc[-1]
            current_price = hist_data['Close'].iloc[-1]
            atr_percent = (atr_value / current_price) * 100
            
            results['atr'] = {
                'value': atr_value, 'percent': atr_percent,
                'signal': 'HIGH VOLATILITY' if atr_percent > 3 else 
                         'LOW VOLATILITY' if atr_percent < 1 else 'NORMAL'
            }
        
        # 52-week high/low proximity
        if ENABLE_52WEEK_ALERTS and len(hist_data) >= 252:
            week52_high = hist_data['Close'].tail(252).max()
            week52_low = hist_data['Close'].tail(252).min()
            current_price = hist_data['Close'].iloc[-1]
            
            distance_from_high = ((current_price - week52_high) / week52_high) * 100
            distance_from_low = ((current_price - week52_low) / week52_low) * 100
            
            results['week52'] = {
                'high': week52_high, 'low': week52_low, 'current': current_price,
                'distance_from_high_pct': distance_from_high,
                'distance_from_low_pct': distance_from_low,
                'signal': 'AT 52W HIGH' if distance_from_high > -2 else 
                         'AT 52W LOW' if distance_from_low < 2 else 
                         'NEAR 52W HIGH' if distance_from_high > -10 else 
                         'NEAR 52W LOW' if distance_from_low < 10 else 'MID-RANGE'
            }
        
        # Gap detection
        if ENABLE_GAP_DETECTION and len(hist_data) >= 2:
            today_open = hist_data['Open'].iloc[-1]
            yesterday_close = hist_data['Close'].iloc[-2]
            gap_percent = ((today_open - yesterday_close) / yesterday_close) * 100
            
            results['gap'] = {
                'gap_percent': gap_percent,
                'signal': 'GAP UP' if gap_percent > 2 else 
                         'GAP DOWN' if gap_percent < -2 else 
                         'SMALL GAP UP' if gap_percent > 0.5 else 
                         'SMALL GAP DOWN' if gap_percent < -0.5 else 'NO GAP'
            }
        
    except Exception as e:
        logging.debug(f"Error in advanced technicals for {ticker}: {e}")
    
    return results

async def get_earnings_countdown(ticker, info):
    """v2.0.0: Earnings date countdown"""
    try:
        if ENABLE_EARNINGS_COUNTDOWN:
            earnings_date = info.get('earningsDate')
            if earnings_date and len(earnings_date) > 0:
                earnings_dt = datetime.datetime.fromtimestamp(earnings_date[0])
                today = datetime.datetime.now()
                days_until = (earnings_dt - today).days
                
                return {
                    'date': earnings_dt.strftime('%Y-%m-%d'),
                    'days_until': days_until,
                    'signal': 'EARNINGS THIS WEEK' if 0 <= days_until <= 7 else 
                             'EARNINGS NEXT WEEK' if 7 < days_until <= 14 else 
                             'EARNINGS SOON' if 14 < days_until <= 30 else 'NO IMMEDIATE EARNINGS'
                }
    except Exception as e:
        logging.debug(f"Could not get earnings date for {ticker}: {e}")
    
    return None

async def analyze_portfolio_with_v2_features(session, portfolio_file='portfolio.json'):
    """v2.0.0: Enhanced portfolio analysis with advanced technicals"""
    logging.info("üìä [v2.0] Analyzing portfolio with advanced features...")
    
    if not os.path.exists(portfolio_file):
        logging.warning(f"Portfolio file {portfolio_file} not found")
        return None
    
    with open(portfolio_file, 'r') as f:
        portfolio_tickers = json.load(f)
    
    portfolio_data = {
        'stocks': [], 'alerts': [], 'opportunities': [], 'risks': [],
        'v2_signals': []  # NEW: v2.0.0 specific high-priority signals
    }
    
    for ticker in portfolio_tickers:
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period="1y", interval="1d")
            info = stock.info
            
            if hist.empty:
                continue
            
            # v1.0.0 stable metrics
            current_price = hist['Close'].iloc[-1]
            prev_close = hist['Close'].iloc[-2] if len(hist) > 1 else current_price
            volume = hist['Volume'].iloc[-1]
            avg_volume = hist['Volume'].mean()
            
            rsi = RSIIndicator(hist['Close'], window=14).rsi().iloc[-1]
            macd = MACD(hist['Close'])
            macd_diff = macd.macd_diff().iloc[-1]
            stoch = StochasticOscillator(hist['High'], hist['Low'], hist['Close'])
            stoch_k = stoch.stoch().iloc[-1]
            
            daily_change = ((current_price - prev_close) / prev_close) * 100
            weekly_change = ((current_price - hist['Close'].iloc[-5]) / hist['Close'].iloc[-5]) * 100 if len(hist) >= 5 else 0
            monthly_change = ((current_price - hist['Close'].iloc[-22]) / hist['Close'].iloc[-22]) * 100 if len(hist) >= 22 else 0
            volume_spike = (volume / avg_volume) if avg_volume > 0 else 1
            
            # v2.0.0 NEW features
            advanced_tech = await analyze_advanced_technicals(ticker, hist)
            earnings_info = await get_earnings_countdown(ticker, info)
            
            stock_analysis = {
                'ticker': ticker, 'name': info.get('shortName', ticker), 'price': current_price,
                'daily_change': daily_change, 'weekly_change': weekly_change, 'monthly_change': monthly_change,
                'rsi': rsi, 'macd': macd_diff, 'stochastic': stoch_k, 'volume_ratio': volume_spike,
                'sector': info.get('sector', 'Unknown'), 'market_cap': info.get('marketCap', 0),
                'pe_ratio': info.get('trailingPE', 0),
                # v2.0.0 additions
                'bollinger': advanced_tech.get('bollinger'),
                'atr': advanced_tech.get('atr'),
                'week52': advanced_tech.get('week52'),
                'gap': advanced_tech.get('gap'),
                'earnings': earnings_info
            }
            
            portfolio_data['stocks'].append(stock_analysis)
            
            # v1.0.0 stable alerts
            if rsi > 70:
                portfolio_data['alerts'].append(f"‚ö†Ô∏è {ticker}: RSI Overbought ({rsi:.1f})")
            elif rsi < 30:
                portfolio_data['opportunities'].append(f"üéØ {ticker}: RSI Oversold ({rsi:.1f})")
            
            if volume_spike > 2:
                portfolio_data['alerts'].append(f"üìä {ticker}: Volume spike ({volume_spike:.1f}x)")
            
            # v2.0.0 NEW alerts - HIGH PRIORITY
            if advanced_tech.get('bollinger') and advanced_tech['bollinger']['squeeze']:
                portfolio_data['v2_signals'].append(f"üí• {ticker}: BOLLINGER SQUEEZE - Breakout imminent (width: {advanced_tech['bollinger']['width']:.1f}%)")
            
            if advanced_tech.get('bollinger'):
                if advanced_tech['bollinger']['position'] > 95:
                    portfolio_data['alerts'].append(f"üìà {ticker}: At upper Bollinger Band - resistance")
                elif advanced_tech['bollinger']['position'] < 5:
                    portfolio_data['opportunities'].append(f"üìâ {ticker}: At lower Bollinger Band - support")
            
            if advanced_tech.get('atr') and advanced_tech['atr']['signal'] == 'HIGH VOLATILITY':
                portfolio_data['risks'].append(f"‚ö° {ticker}: HIGH VOLATILITY ({advanced_tech['atr']['percent']:.1f}%) - use wider stops")
            
            if advanced_tech.get('week52'):
                if advanced_tech['week52']['signal'] == 'AT 52W HIGH':
                    portfolio_data['v2_signals'].append(f"üöÄ {ticker}: AT 52-WEEK HIGH (${advanced_tech['week52']['high']:.2f}) - strong momentum")
                elif advanced_tech['week52']['signal'] == 'AT 52W LOW':
                    portfolio_data['opportunities'].append(f"üíé {ticker}: AT 52-WEEK LOW (${advanced_tech['week52']['low']:.2f}) - potential reversal")
                elif advanced_tech['week52']['signal'] == 'NEAR 52W HIGH':
                    portfolio_data['v2_signals'].append(f"üìà {ticker}: Near 52W high ({advanced_tech['week52']['distance_from_high_pct']:.1f}% away)")
            
            if advanced_tech.get('gap'):
                if advanced_tech['gap']['signal'] == 'GAP UP':
                    portfolio_data['v2_signals'].append(f"‚¨ÜÔ∏è {ticker}: GAPPED UP {advanced_tech['gap']['gap_percent']:.1f}% - strong buying")
                elif advanced_tech['gap']['signal'] == 'GAP DOWN':
                    portfolio_data['alerts'].append(f"‚¨áÔ∏è {ticker}: GAPPED DOWN {advanced_tech['gap']['gap_percent']:.1f}% - watch support")
            
            if earnings_info:
                if earnings_info['signal'] == 'EARNINGS THIS WEEK':
                    portfolio_data['v2_signals'].append(f"üìÖ {ticker}: EARNINGS IN {earnings_info['days_until']} DAYS ({earnings_info['date']}) - expect volatility")
                elif earnings_info['signal'] == 'EARNINGS NEXT WEEK':
                    portfolio_data['alerts'].append(f"üìÖ {ticker}: Earnings in {earnings_info['days_until']} days ({earnings_info['date']})")
            
            # Skip insider for Canadian stocks
            if FINNHUB_KEY and not ticker.endswith('.TO'):
                insider_url = f"https://finnhub.io/api/v1/stock/insider-transactions?symbol={ticker}&token={FINNHUB_KEY}"
                insider_content = await make_robust_request(session, insider_url)
                if insider_content:
                    transactions = json.loads(insider_content).get('data', [])
                    recent_buys = [t for t in transactions[:5] if t.get('transactionType') == 'Buy']
                    recent_sells = [t for t in transactions[:5] if t.get('transactionType') == 'Sell']
                    if recent_buys:
                        portfolio_data['alerts'].append(f"üí∞ {ticker}: Insider buying detected")
                    if len(recent_sells) > len(recent_buys) * 2:
                        portfolio_data['risks'].append(f"‚ö†Ô∏è {ticker}: Heavy insider selling")
            
        except Exception as e:
            logging.warning(f"Error analyzing {ticker}: {e}")
            continue
    
    return portfolio_data

async def generate_portfolio_recommendations_from_pattern(portfolio_data, pattern_data, macro_data):
    """FIXED v2.0.0: Generate CLEAR, NON-CONFLICTING recommendations"""
    if not pattern_data or not pattern_data.get('matches') or not portfolio_data:
        return None
    
    recommendations = {
        'final_verdicts': {},  # ONE recommendation per stock
        'sector_winners': [],
        'sector_losers': []
    }
    
    # Get sector performance data
    if pattern_data.get('sector_performance'):
        recommendations['sector_winners'] = pattern_data['sector_performance'][:3]
        recommendations['sector_losers'] = pattern_data['sector_performance'][-3:]
    
    # Create sector ranking for better logic
    sector_ranks = {}
    if pattern_data.get('sector_performance'):
        for idx, (sector, perf) in enumerate(pattern_data['sector_performance']):
            sector_ranks[sector] = {
                'rank': idx + 1,
                'performance': perf,
                'is_winner': idx < 3,
                'is_loser': idx >= len(pattern_data['sector_performance']) - 3
            }
    
    # Analyze each portfolio stock with PRIORITY-BASED system
    for stock in portfolio_data['stocks']:
        ticker = stock['ticker']
        sector = stock.get('sector', 'Unknown')
        rsi = stock['rsi']
        monthly_change = stock['monthly_change']
        
        # Priority scoring system (higher priority = stronger signal)
        signals = []
        
        # PRIORITY 1: Extreme RSI (strongest signal)
        if rsi > 80:
            signals.append({
                'priority': 10,
                'action': 'SELL',
                'reason': f'Extremely overbought (RSI {rsi:.0f})',
                'confidence': 'HIGH'
            })
        elif rsi > 70 and monthly_change > 30:
            signals.append({
                'priority': 9,
                'action': 'TAKE PROFITS',
                'reason': f'Overbought (RSI {rsi:.0f}) + Extended rally (+{monthly_change:.0f}%)',
                'confidence': 'HIGH'
            })
        elif rsi < 20:
            signals.append({
                'priority': 10,
                'action': 'BUY',
                'reason': f'Extremely oversold (RSI {rsi:.0f})',
                'confidence': 'HIGH'
            })
        elif rsi < 30:
            signals.append({
                'priority': 8,
                'action': 'BUY DIP',
                'reason': f'Oversold (RSI {rsi:.0f}) - bounce likely',
                'confidence': 'MEDIUM'
            })
        
        # PRIORITY 2: Extreme price moves
        if monthly_change > 50:
            signals.append({
                'priority': 8,
                'action': 'TRIM 50%',
                'reason': f'Parabolic move (+{monthly_change:.0f}% monthly)',
                'confidence': 'HIGH'
            })
        elif monthly_change < -30:
            signals.append({
                'priority': 7,
                'action': 'AVERAGE DOWN',
                'reason': f'Heavy selloff ({monthly_change:.0f}% monthly)',
                'confidence': 'MEDIUM'
            })
        
        # PRIORITY 3: Technical + Pattern alignment
        if stock.get('gap') and stock['gap']['signal'] == 'GAP UP':
            if rsi < 60:
                signals.append({
                    'priority': 6,
                    'action': 'HOLD',
                    'reason': f"Gapped up {stock['gap']['gap_percent']:.1f}% with room to run",
                    'confidence': 'MEDIUM'
                })
            else:
                signals.append({
                    'priority': 5,
                    'action': 'WATCH',
                    'reason': f"Gapped up but getting extended",
                    'confidence': 'LOW'
                })
        
        # PRIORITY 4: 52-week levels
        if stock.get('week52'):
            if stock['week52']['signal'] == 'AT 52W HIGH' and rsi < 70:
                signals.append({
                    'priority': 5,
                    'action': 'HOLD',
                    'reason': 'At 52W high with momentum',
                    'confidence': 'MEDIUM'
                })
            elif stock['week52']['signal'] == 'AT 52W LOW':
                signals.append({
                    'priority': 6,
                    'action': 'BUY STARTER',
                    'reason': 'At 52W low - potential reversal',
                    'confidence': 'MEDIUM'
                })
        
        # PRIORITY 5: Sector performance
        if sector in sector_ranks:
            rank_info = sector_ranks[sector]
            if rank_info['is_winner'] and rsi < 60:
                signals.append({
                    'priority': 4,
                    'action': 'ADD',
                    'reason': f"{sector} ranked #{rank_info['rank']} (+{rank_info['performance']:.1f}%) historically",
                    'confidence': 'LOW'
                })
            elif rank_info['is_loser'] and rsi > 50:
                signals.append({
                    'priority': 3,
                    'action': 'REDUCE',
                    'reason': f"{sector} underperformed (ranked #{rank_info['rank']})",
                    'confidence': 'LOW'
                })
        
        # PRIORITY 6: Historical pattern baseline
        if pattern_data['avg_return_3m'] > 5 and not signals:
            signals.append({
                'priority': 2,
                'action': 'HOLD',
                'reason': f"Pattern bullish (+{pattern_data['avg_return_3m']:.1f}% avg)",
                'confidence': 'LOW'
            })
        elif pattern_data['avg_return_3m'] < -5 and not signals:
            signals.append({
                'priority': 2,
                'action': 'HEDGE',
                'reason': f"Pattern bearish ({pattern_data['avg_return_3m']:.1f}% avg)",
                'confidence': 'LOW'
            })
        
        # DEFAULT: No strong signals
        if not signals:
            signals.append({
                'priority': 1,
                'action': 'HOLD',
                'reason': 'No strong signals - maintain position',
                'confidence': 'NEUTRAL'
            })
        
        # Pick the HIGHEST PRIORITY signal as final verdict
        final_signal = max(signals, key=lambda x: x['priority'])
        
        recommendations['final_verdicts'][ticker] = {
            'action': final_signal['action'],
            'reason': final_signal['reason'],
            'confidence': final_signal['confidence'],
            'name': stock['name']
        }
    
    return recommendations

# ========================================
# üß† INTELLIGENT LEARNING SYSTEM v3.0
# 100% Free, GitHub Actions Compatible
# ========================================

import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path

class PredictionTracker:
    """Tracks all predictions and learns from outcomes - JSON based for free storage"""
    
    def __init__(self, predictions_file='data/predictions.json'):
        self.predictions_file = Path(predictions_file)
        self.predictions_file.parent.mkdir(exist_ok=True)
        self.predictions = self._load_predictions()
        
    def _load_predictions(self):
        """Load existing predictions from JSON"""
        if self.predictions_file.exists():
            try:
                with open(self.predictions_file, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_predictions(self):
        """Save predictions to JSON"""
        with open(self.predictions_file, 'w') as f:
            json.dump(self.predictions, f, indent=2, default=str)
    
    def store_prediction(self, ticker, action, confidence, reasoning, candle_pattern=None, indicators=None):
        """Store a new prediction with all context"""
        prediction_id = hashlib.md5(f"{ticker}{datetime.now().isoformat()}".encode()).hexdigest()[:8]
        
        prediction = {
            'id': prediction_id,
            'timestamp': datetime.now().isoformat(),
            'ticker': ticker,
            'action': action,  # BUY, SELL, HOLD
            'confidence': confidence,  # 0-100
            'reasoning': reasoning,
            'candle_pattern': candle_pattern,
            'indicators': indicators or {},
            'price_at_prediction': None,  # Will be filled
            'outcome': None,  # Will be updated later
            'was_correct': None  # Will be calculated
        }
        
        # Get current price
        try:
            import yfinance as yf
            current_price = yf.Ticker(ticker).history(period='1d')['Close'].iloc[-1]
            prediction['price_at_prediction'] = float(current_price)
        except:
            pass
        
        self.predictions[prediction_id] = prediction
        self._save_predictions()
        logging.info(f"üìù Stored prediction {prediction_id}: {ticker} - {action} (confidence: {confidence}%)")
        return prediction_id
    
    def check_outcomes(self, days_to_check=1):
        """Check outcomes of past predictions"""
        results = {'checked': 0, 'correct': 0, 'wrong': 0}
        cutoff_date = datetime.now() - timedelta(days=days_to_check)
        
        for pred_id, pred in self.predictions.items():
            if pred['outcome'] is not None:  # Already checked
                continue
                
            pred_date = datetime.fromisoformat(pred['timestamp'])
            if pred_date > cutoff_date:  # Too recent
                continue
            
            try:
                ticker = pred['ticker']
                stock = yf.Ticker(ticker)
                
                # Get price from prediction date to now
                hist = stock.history(start=pred_date.date(), end=datetime.now().date())
                if len(hist) < 2:
                    continue
                
                current_price = hist['Close'].iloc[-1]
                pred_price = pred['price_at_prediction']
                
                if not pred_price:
                    continue
                
                price_change_pct = ((current_price - pred_price) / pred_price) * 100
                
                # Determine if prediction was correct
                was_correct = False
                if pred['action'] == 'BUY' and price_change_pct > 0.5:
                    was_correct = True
                elif pred['action'] == 'SELL' and price_change_pct < -0.5:
                    was_correct = True
                elif pred['action'] == 'HOLD' and abs(price_change_pct) < 2:
                    was_correct = True
                
                # Update prediction
                pred['outcome'] = {
                    'checked_date': datetime.now().isoformat(),
                    'price_change_pct': price_change_pct,
                    'current_price': float(current_price)
                }
                pred['was_correct'] = was_correct
                
                results['checked'] += 1
                if was_correct:
                    results['correct'] += 1
                else:
                    results['wrong'] += 1
                    
                logging.info(f"‚úÖ Outcome: {ticker} {pred['action']} was {'CORRECT' if was_correct else 'WRONG'} ({price_change_pct:+.2f}%)")
                
            except Exception as e:
                logging.error(f"Error checking outcome for {pred_id}: {e}")
        
        self._save_predictions()
        return results


class CandlePatternAnalyzer:
    """Identifies 20+ candlestick patterns with historical success tracking"""
    
    def __init__(self, patterns_file='data/patterns.json'):
        self.patterns_file = Path(patterns_file)
        self.patterns_file.parent.mkdir(exist_ok=True)
        self.pattern_history = self._load_patterns()
    
    def _load_patterns(self):
        """Load pattern success history"""
        if self.patterns_file.exists():
            try:
                with open(self.patterns_file, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_patterns(self):
        """Save pattern history"""
        with open(self.patterns_file, 'w') as f:
            json.dump(self.pattern_history, f, indent=2)
    
    def identify_pattern(self, hist_data, min_length=3):
        """
        Identify candlestick patterns from historical data
        hist_data: DataFrame with OHLC columns
        Returns: List of identified patterns with metadata
        """
        if len(hist_data) < min_length:
            return []
        
        patterns = []
        
        # Get last 3 candles for pattern detection
        candles = []
        for i in range(min(3, len(hist_data))):
            idx = -(i+1)
            candles.append({
                'open': float(hist_data['Open'].iloc[idx]),
                'high': float(hist_data['High'].iloc[idx]),
                'low': float(hist_data['Low'].iloc[idx]),
                'close': float(hist_data['Close'].iloc[idx]),
            })
        
        if len(candles) < 1:
            return []
        
        # Current candle (today)
        c0 = candles[0]
        body0 = abs(c0['close'] - c0['open'])
        range0 = c0['high'] - c0['low'] if c0['high'] != c0['low'] else 0.01
        body_ratio0 = body0 / range0 if range0 > 0 else 0
        
        # Previous candle (if exists)
        c1 = candles[1] if len(candles) > 1 else None
        
        # 2 candles ago (if exists)
        c2 = candles[2] if len(candles) > 2 else None
        
        # ===== REVERSAL PATTERNS (BULLISH) =====
        
        # 1. Hammer (bullish reversal)
        if (min(c0['open'], c0['close']) - c0['low']) > body0 * 2:
            if (c0['high'] - max(c0['open'], c0['close'])) < body0:
                patterns.append({
                    'name': 'hammer',
                    'type': 'bullish_reversal',
                    'strength': 'strong',
                    'description': 'Hammer - potential bottom reversal'
                })
        
        # 2. Inverted Hammer (bullish reversal)
        if (c0['high'] - max(c0['open'], c0['close'])) > body0 * 2:
            if (min(c0['open'], c0['close']) - c0['low']) < body0:
                patterns.append({
                    'name': 'inverted_hammer',
                    'type': 'bullish_reversal',
                    'strength': 'medium',
                    'description': 'Inverted Hammer - potential reversal'
                })
        
        # 3. Bullish Engulfing (requires 2 candles)
        if c1:
            if c1['close'] < c1['open']:  # Previous was bearish
                if c0['close'] > c0['open']:  # Current is bullish
                    if c0['open'] < c1['close'] and c0['close'] > c1['open']:
                        patterns.append({
                            'name': 'bullish_engulfing',
                            'type': 'bullish_reversal',
                            'strength': 'strong',
                            'description': 'Bullish Engulfing - strong reversal signal'
                        })
        
        # 4. Piercing Line (requires 2 candles)
        if c1:
            if c1['close'] < c1['open']:  # Previous was bearish
                if c0['close'] > c0['open']:  # Current is bullish
                    mid_c1 = (c1['open'] + c1['close']) / 2
                    if c0['open'] < c1['close'] and c0['close'] > mid_c1:
                        patterns.append({
                            'name': 'piercing_line',
                            'type': 'bullish_reversal',
                            'strength': 'medium',
                            'description': 'Piercing Line - bullish reversal'
                        })
        
        # 5. Morning Star (requires 3 candles)
        if c1 and c2:
            if c2['close'] < c2['open']:  # Day 1: bearish
                body1 = abs(c1['close'] - c1['open'])
                if body1 < body0 * 0.3:  # Day 2: small body (star)
                    if c0['close'] > c0['open']:  # Day 3: bullish
                        if c0['close'] > (c2['open'] + c2['close']) / 2:
                            patterns.append({
                                'name': 'morning_star',
                                'type': 'bullish_reversal',
                                'strength': 'very_strong',
                                'description': 'Morning Star - major reversal signal'
                            })
        
        # 6. Three White Soldiers (requires 3 candles)
        if c1 and c2:
            if (c0['close'] > c0['open'] and 
                c1['close'] > c1['open'] and 
                c2['close'] > c2['open']):
                if c0['close'] > c1['close'] > c2['close']:
                    patterns.append({
                        'name': 'three_white_soldiers',
                        'type': 'bullish_continuation',
                        'strength': 'strong',
                        'description': 'Three White Soldiers - strong uptrend'
                    })
        
        # ===== REVERSAL PATTERNS (BEARISH) =====
        
        # 7. Hanging Man (bearish reversal)
        if c0['close'] > c0['open']:  # Bullish candle
            if (min(c0['open'], c0['close']) - c0['low']) > body0 * 2:
                if (c0['high'] - max(c0['open'], c0['close'])) < body0:
                    # Check if in uptrend (price higher than 5 days ago)
                    if len(hist_data) >= 6:
                        if c0['close'] > hist_data['Close'].iloc[-6]:
                            patterns.append({
                                'name': 'hanging_man',
                                'type': 'bearish_reversal',
                                'strength': 'medium',
                                'description': 'Hanging Man - potential top reversal'
                            })
        
        # 8. Shooting Star (bearish reversal)
        if (c0['high'] - max(c0['open'], c0['close'])) > body0 * 2:
            if (min(c0['open'], c0['close']) - c0['low']) < body0:
                # Works best after uptrend
                if len(hist_data) >= 6:
                    if c0['close'] > hist_data['Close'].iloc[-6]:
                        patterns.append({
                            'name': 'shooting_star',
                            'type': 'bearish_reversal',
                            'strength': 'strong',
                            'description': 'Shooting Star - reversal warning'
                        })
        
        # 9. Bearish Engulfing (requires 2 candles)
        if c1:
            if c1['close'] > c1['open']:  # Previous was bullish
                if c0['close'] < c0['open']:  # Current is bearish
                    if c0['open'] > c1['close'] and c0['close'] < c1['open']:
                        patterns.append({
                            'name': 'bearish_engulfing',
                            'type': 'bearish_reversal',
                            'strength': 'strong',
                            'description': 'Bearish Engulfing - strong reversal signal'
                        })
        
        # 10. Dark Cloud Cover (requires 2 candles)
        if c1:
            if c1['close'] > c1['open']:  # Previous was bullish
                if c0['close'] < c0['open']:  # Current is bearish
                    mid_c1 = (c1['open'] + c1['close']) / 2
                    if c0['open'] > c1['high'] and c0['close'] < mid_c1:
                        patterns.append({
                            'name': 'dark_cloud_cover',
                            'type': 'bearish_reversal',
                            'strength': 'medium',
                            'description': 'Dark Cloud Cover - bearish reversal'
                        })
        
        # 11. Evening Star (requires 3 candles)
        if c1 and c2:
            if c2['close'] > c2['open']:  # Day 1: bullish
                body1 = abs(c1['close'] - c1['open'])
                if body1 < body0 * 0.3:  # Day 2: small body (star)
                    if c0['close'] < c0['open']:  # Day 3: bearish
                        if c0['close'] < (c2['open'] + c2['close']) / 2:
                            patterns.append({
                                'name': 'evening_star',
                                'type': 'bearish_reversal',
                                'strength': 'very_strong',
                                'description': 'Evening Star - major reversal signal'
                            })
        
        # 12. Three Black Crows (requires 3 candles)
        if c1 and c2:
            if (c0['close'] < c0['open'] and 
                c1['close'] < c1['open'] and 
                c2['close'] < c2['open']):
                if c0['close'] < c1['close'] < c2['close']:
                    patterns.append({
                        'name': 'three_black_crows',
                        'type': 'bearish_continuation',
                        'strength': 'strong',
                        'description': 'Three Black Crows - strong downtrend'
                    })
        
        # ===== CONTINUATION & INDECISION PATTERNS =====
        
        # 13. Doji (indecision)
        if body_ratio0 < 0.1:
            patterns.append({
                'name': 'doji',
                'type': 'indecision',
                'strength': 'weak',
                'description': 'Doji - market indecision'
            })
        
        # 14. Spinning Top (indecision)
        if 0.1 < body_ratio0 < 0.3:
            if (c0['high'] - max(c0['open'], c0['close'])) > body0:
                if (min(c0['open'], c0['close']) - c0['low']) > body0:
                    patterns.append({
                        'name': 'spinning_top',
                        'type': 'indecision',
                        'strength': 'weak',
                        'description': 'Spinning Top - indecision'
                    })
        
        # 15. Rising Three Methods (bullish continuation - requires 5 candles)
        if len(hist_data) >= 5:
            candles_5 = []
            for i in range(5):
                idx = -(i+1)
                candles_5.append({
                    'open': float(hist_data['Open'].iloc[idx]),
                    'close': float(hist_data['Close'].iloc[idx]),
                })
            
            # Day 1: Long bullish
            if candles_5[4]['close'] > candles_5[4]['open']:
                # Days 2-4: Small bearish candles within day 1 range
                small_pullback = all(
                    candles_5[i]['close'] < candles_5[i]['open'] and
                    candles_5[i]['close'] > candles_5[4]['close']
                    for i in [3, 2, 1]
                )
                # Day 5: Bullish continuation
                if small_pullback and candles_5[0]['close'] > candles_5[0]['open']:
                    if candles_5[0]['close'] > candles_5[4]['close']:
                        patterns.append({
                            'name': 'rising_three_methods',
                            'type': 'bullish_continuation',
                            'strength': 'medium',
                            'description': 'Rising Three Methods - uptrend continues'
                        })
        
        # 16. Falling Three Methods (bearish continuation - requires 5 candles)
        if len(hist_data) >= 5:
            candles_5 = []
            for i in range(5):
                idx = -(i+1)
                candles_5.append({
                    'open': float(hist_data['Open'].iloc[idx]),
                    'close': float(hist_data['Close'].iloc[idx]),
                })
            
            # Day 1: Long bearish
            if candles_5[4]['close'] < candles_5[4]['open']:
                # Days 2-4: Small bullish candles within day 1 range
                small_bounce = all(
                    candles_5[i]['close'] > candles_5[i]['open'] and
                    candles_5[i]['close'] < candles_5[4]['close']
                    for i in [3, 2, 1]
                )
                # Day 5: Bearish continuation
                if small_bounce and candles_5[0]['close'] < candles_5[0]['open']:
                    if candles_5[0]['close'] < candles_5[4]['close']:
                        patterns.append({
                            'name': 'falling_three_methods',
                            'type': 'bearish_continuation',
                            'strength': 'medium',
                            'description': 'Falling Three Methods - downtrend continues'
                        })
        
        # ===== ADDITIONAL PATTERNS =====
        
        # 17. Marubozu (strong direction)
        if body0 > range0 * 0.95:  # Almost no wicks
            if c0['close'] > c0['open']:
                patterns.append({
                    'name': 'bullish_marubozu',
                    'type': 'bullish_continuation',
                    'strength': 'strong',
                    'description': 'Bullish Marubozu - strong buying'
                })
            else:
                patterns.append({
                    'name': 'bearish_marubozu',
                    'type': 'bearish_continuation',
                    'strength': 'strong',
                    'description': 'Bearish Marubozu - strong selling'
                })
        
        # 18. Tweezer Top/Bottom (requires 2 candles)
        if c1:
            # Tweezer Top (bearish)
            if abs(c0['high'] - c1['high']) < range0 * 0.02:  # Similar highs
                if c1['close'] > c1['open'] and c0['close'] < c0['open']:
                    patterns.append({
                        'name': 'tweezer_top',
                        'type': 'bearish_reversal',
                        'strength': 'medium',
                        'description': 'Tweezer Top - potential reversal'
                    })
            
            # Tweezer Bottom (bullish)
            if abs(c0['low'] - c1['low']) < range0 * 0.02:  # Similar lows
                if c1['close'] < c1['open'] and c0['close'] > c0['open']:
                    patterns.append({
                        'name': 'tweezer_bottom',
                        'type': 'bullish_reversal',
                        'strength': 'medium',
                        'description': 'Tweezer Bottom - potential reversal'
                    })
        
        # Remove duplicates and return
        unique_patterns = []
        seen = set()
        for p in patterns:
            if p['name'] not in seen:
                unique_patterns.append(p)
                seen.add(p['name'])
        
        return unique_patterns
    
    def get_pattern_success_rate(self, pattern_name, ticker=None):
        """Get historical success rate for a pattern"""
        key = f"{ticker}_{pattern_name}" if ticker else pattern_name
        
        # Try ticker-specific first
        if key in self.pattern_history:
            stats = self.pattern_history[key]
            total = stats.get('total', 0)
            successful = stats.get('successful', 0)
            if total > 3:  # Need at least 3 occurrences for reliability
                return (successful / total) * 100
        
        # Fall back to global pattern stats
        if pattern_name in self.pattern_history:
            stats = self.pattern_history[pattern_name]
            total = stats.get('total', 0)
            successful = stats.get('successful', 0)
            if total > 0:
                return (successful / total) * 100
        
        # Default expectations based on pattern type
        defaults = {
            'bullish_reversal': 65.0,
            'bearish_reversal': 65.0,
            'bullish_continuation': 60.0,
            'bearish_continuation': 60.0,
            'indecision': 50.0
        }
        
        # Try to guess from pattern name
        for pattern_type, default_rate in defaults.items():
            if pattern_type in pattern_name:
                return default_rate
        
        return 50.0  # Neutral default
    
    def update_pattern_outcome(self, pattern_name, ticker, was_successful):
        """Update pattern success history after checking outcome"""
        # Update ticker-specific stats
        key = f"{ticker}_{pattern_name}"
        if key not in self.pattern_history:
            self.pattern_history[key] = {'total': 0, 'successful': 0}
        
        self.pattern_history[key]['total'] += 1
        if was_successful:
            self.pattern_history[key]['successful'] += 1
        
        # Also update global pattern stats
        if pattern_name not in self.pattern_history:
            self.pattern_history[pattern_name] = {'total': 0, 'successful': 0}
        
        self.pattern_history[pattern_name]['total'] += 1
        if was_successful:
            self.pattern_history[pattern_name]['successful'] += 1
        
        self._save_patterns()


class LearningMemory:
    """System memory that improves over time"""
    
    def __init__(self, memory_file='data/learning_memory.json'):
        self.memory_file = Path(memory_file)
        self.memory_file.parent.mkdir(exist_ok=True)
        self.memory = self._load_memory()
    
    def _load_memory(self):
        """Load system memory"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        
        # Initialize with default structure
        return {
            'llm_accuracy': {
                'groq': {'total': 0, 'correct': 0},
                'gemini': {'total': 0, 'correct': 0}
            },
            'indicator_reliability': {
                'rsi_oversold_buy': {'total': 0, 'successful': 0},
                'rsi_overbought_sell': {'total': 0, 'successful': 0},
                'macd_crossover': {'total': 0, 'successful': 0}
            },
            'market_conditions': {
                'high_vix_predictions': {'total': 0, 'correct': 0},
                'low_vix_predictions': {'total': 0, 'correct': 0}
            },
            'insights': []
        }
    
    def _save_memory(self):
        """Save memory to file"""
        with open(self.memory_file, 'w') as f:
            json.dump(self.memory, f, indent=2)
    
    def update_llm_accuracy(self, llm_name, was_correct):
        """Track LLM prediction accuracy"""
        if llm_name not in self.memory['llm_accuracy']:
            self.memory['llm_accuracy'][llm_name] = {'total': 0, 'correct': 0}
        
        self.memory['llm_accuracy'][llm_name]['total'] += 1
        if was_correct:
            self.memory['llm_accuracy'][llm_name]['correct'] += 1
        
        self._save_memory()
    
    def get_llm_weights(self):
        """Get reliability weights for each LLM based on past performance"""
        weights = {}
        for llm, stats in self.memory['llm_accuracy'].items():
            if stats['total'] > 0:
                accuracy = stats['correct'] / stats['total']
                weights[llm] = max(0.1, accuracy)  # Minimum weight of 0.1
            else:
                weights[llm] = 0.5  # Default weight
        
        # Normalize weights
        total = sum(weights.values())
        if total > 0:
            weights = {k: v/total for k, v in weights.items()}
        
        return weights
    
    def add_insight(self, insight):
        """Store a learned insight"""
        self.memory['insights'].append({
            'timestamp': datetime.now().isoformat(),
            'insight': insight
        })
        # Keep only last 100 insights
        self.memory['insights'] = self.memory['insights'][-100:]
        self._save_memory()
    
    def get_recent_insights(self, count=10):
        """Get recent learned insights"""
        return self.memory['insights'][-count:]

# Initialize the learning system components
prediction_tracker = PredictionTracker()
candle_analyzer = CandlePatternAnalyzer()
learning_memory = LearningMemory()
enhanced_pattern_detector = None
# ========================================
# üîó INTEGRATION LAYER - Connects to existing code
# This READS from your existing functions without changing them
# ========================================

class ConfidenceScorer:
    """Calculates conviction score (0-100) for predictions"""
    
    @staticmethod
    def calculate_confidence(
        llm_predictions,
        candle_patterns,
        pattern_success_rates,
        technical_indicators,
        volume_data,
        market_context=None
    ):
        """
        Calculate comprehensive confidence score
        Returns: dict with score, breakdown, and action threshold
        """
        confidence = 50  # Start neutral
        breakdown = []
        
        # 1. LLM CONSENSUS (0-30 points)
        llm_score = 0
        if llm_predictions:
            actions = [p['action'] for p in llm_predictions.values()]
            confidences = [p['confidence'] for p in llm_predictions.values()]
            
            # Agreement boost
            if len(set(actions)) == 1:  # All agree
                llm_score = 30
                breakdown.append(f"‚úÖ All {len(actions)} LLMs agree ({actions[0]}): +30")
            elif len(actions) >= 2:
                most_common = max(set(actions), key=actions.count)
                agreement_pct = (actions.count(most_common) / len(actions)) * 100
                llm_score = int(agreement_pct * 0.3)  # Up to 30 points
                breakdown.append(f"‚úÖ ‚öñÔ∏è {agreement_pct:.0f}% LLM agreement: +{llm_score}")
            
            # Average LLM confidence
            avg_llm_conf = sum(confidences) / len(confidences)
            if avg_llm_conf > 70:
                llm_score += 10
                breakdown.append(f"‚úÖ üéØ High LLM confidence ({avg_llm_conf:.0f}%): +10")
            elif avg_llm_conf < 40:
                llm_score -= 10
                breakdown.append(f"‚ö†Ô∏è Low LLM confidence ({avg_llm_conf:.0f}%): -10")
        
        confidence += llm_score
        
        # ========================================================================
        # 2. CANDLESTICK PATTERN STRENGTH (0-35 points) - ENHANCED VERSION
        # ========================================================================
        pattern_score = 0
        
        # Separate basic and enhanced patterns
        basic_patterns = [p for p in candle_patterns if not p.get('enhanced', False)]
        enhanced_patterns = [p for p in candle_patterns if p.get('enhanced', False)]
        
        # Score ENHANCED patterns first (they're more sophisticated)
        if enhanced_patterns:
            # Get the strongest enhanced pattern
            best_enhanced = max(enhanced_patterns, key=lambda x: x.get('strength_score', 0))
            strength_score = best_enhanced.get('strength_score', 0)
            
            if strength_score >= 85:
                pattern_score += 20
                breakdown.append(f"‚úÖ üî• {best_enhanced['name'].replace('_', ' ').title()} ({strength_score}% strength): +20")
            elif strength_score >= 75:
                pattern_score += 15
                breakdown.append(f"‚úÖ üìà {best_enhanced['name'].replace('_', ' ').title()} ({strength_score}% strength): +15")
            elif strength_score >= 65:
                pattern_score += 10
                breakdown.append(f"‚úÖ ‚û°Ô∏è {best_enhanced['name'].replace('_', ' ').title()} ({strength_score}% strength): +10")
            
            # Bonus for multiple enhanced patterns confirming
            if len(enhanced_patterns) >= 2:
                pattern_score += 5
                breakdown.append(f"‚úÖ üéØ Multiple patterns confirm ({len(enhanced_patterns)} found): +5")
        
        # Score BASIC patterns (if no enhanced or as supplement)
        elif basic_patterns:
            strong_patterns = [p for p in basic_patterns if p.get('strength') in ['strong', 'very_strong']]
            
            if strong_patterns:
                # Get best pattern success rate
                best_success_rate = 0
                best_pattern = None
                for pattern in strong_patterns:
                    rate = pattern_success_rates.get(pattern['name'], 50)
                    if rate > best_success_rate:
                        best_success_rate = rate
                        best_pattern = pattern
                
                if best_success_rate > 70:
                    pattern_score = 25
                    breakdown.append(f"‚úÖ üìà Strong {best_pattern['name']} ({best_success_rate:.0f}% success): +25")
                elif best_success_rate > 60:
                    pattern_score = 15
                    breakdown.append(f"‚úÖ üìä {best_pattern['name']} ({best_success_rate:.0f}% success): +15")
                elif best_success_rate > 50:
                    pattern_score = 10
                    breakdown.append(f"‚úÖ ‚û°Ô∏è {best_pattern['name']} ({best_success_rate:.0f}% success): +10")
        
        # Penalty for conflicting patterns (bullish vs bearish)
        pattern_types = [p.get('type', '') for p in candle_patterns]
        if 'bullish_reversal' in pattern_types and 'bearish_reversal' in pattern_types:
            pattern_score -= 15
            breakdown.append(f"‚ö†Ô∏è ‚ö†Ô∏è Conflicting patterns: -15")
        elif 'bullish_continuation' in pattern_types and 'bearish_continuation' in pattern_types:
            pattern_score -= 10
            breakdown.append(f"‚ö†Ô∏è ‚ö†Ô∏è Mixed trend signals: -10")
        
        confidence += pattern_score
        # ========================================================================
        
        # 3. TECHNICAL INDICATORS (0-20 points)
        indicator_score = 0
        if technical_indicators:
            rsi = technical_indicators.get('rsi', 50)
            macd = technical_indicators.get('macd', 0)
            score = technical_indicators.get('score', 50)
            
            # RSI alignment
            if 30 < rsi < 70:
                indicator_score += 10
                breakdown.append(f"‚úÖ ‚úÖ RSI balanced ({rsi:.0f}): +10")
            elif rsi < 30:
                indicator_score += 5
                breakdown.append(f"‚úÖ üîµ RSI oversold ({rsi:.0f}): +5")
            elif rsi > 70:
                indicator_score -= 5
                breakdown.append(f"‚ö†Ô∏è üî¥ RSI overbought ({rsi:.0f}): -5")
            
            # System score
            if score > 70:
                indicator_score += 10
                breakdown.append(f"‚úÖ ‚≠ê High system score ({score:.0f}): +10")
            elif score < 40:
                indicator_score -= 10
                breakdown.append(f"‚ö†Ô∏è ‚ö†Ô∏è Low system score ({score:.0f}): -10")
        
        confidence += indicator_score
        
        # 4. VOLUME CONFIRMATION (0-15 points)
        volume_score = 0
        if volume_data:
            volume_ratio = volume_data.get('volume_ratio', 1.0)
            
            if volume_ratio > 2.0:
                volume_score = 15
                breakdown.append(f"‚úÖ üìä High volume ({volume_ratio:.1f}x avg): +15")
            elif volume_ratio > 1.5:
                volume_score = 10
                breakdown.append(f"‚úÖ üìà Above avg volume ({volume_ratio:.1f}x): +10")
            elif volume_ratio < 0.5:
                volume_score = -10
                breakdown.append(f"‚ö†Ô∏è üìâ Low volume ({volume_ratio:.1f}x avg): -10")
        
        confidence += volume_score
        
        # 5. MARKET CONTEXT (0-10 points)
        context_score = 0
        if market_context:
            macro_score = market_context.get('overall_macro_score', 0)
            
            if macro_score > 10:
                context_score = 10
                breakdown.append(f"‚úÖ üåç Positive market context (+{macro_score:.0f}): +10")
            elif macro_score < -10:
                context_score = -10
                breakdown.append(f"‚ö†Ô∏è ‚ö†Ô∏è Negative market context ({macro_score:.0f}): -10")
        
        confidence += context_score
        
        # Cap confidence between 0-100
        confidence = max(0, min(100, confidence))

                # ========================================================================
        # 6. NEWS/EVENTS SCORING (NEW - DOESN'T TOUCH EXISTING CODE)
        # ========================================================================
        events_score = 0
        
        # Try to load news events engine
        try:
            from analysis.news_events_engine import NewsEventsEngine
            
            # Get ticker from technical_indicators or predictions
            ticker = None
            if technical_indicators and 'ticker' in technical_indicators:
                ticker = technical_indicators['ticker']
            elif llm_predictions:
                # Try to extract ticker from LLM predictions
                first_pred = next(iter(llm_predictions.values()), {})
                ticker = first_pred.get('ticker')
            
            if ticker:
                engine = NewsEventsEngine()
                events_data = engine.analyze_events(ticker, days_back=7)
                
                if events_data and events_data.get('event_count', 0) > 0:
                    events_score = events_data['total_score']
                    
                    # Add events to breakdown
                    for event in events_data.get('events', []):
                        emoji = event.get('emoji', 'üì∞')
                        desc = event.get('description', '')
                        score = event.get('score', 0)
                        
                        if score > 0:
                            breakdown.append(f"‚úÖ {emoji} {desc}: +{score}")
                        elif score < 0:
                            breakdown.append(f"‚ö†Ô∏è {emoji} {desc}: {score}")
                    
                    # Log major events
                    if events_data.get('has_major_events'):
                        logging.info(f"üî• {ticker}: Major market event detected! Score: {events_score:+d}")
        
        except ImportError:
            logging.debug("News events engine not available")
        except Exception as e:
            logging.debug(f"Error in news events scoring: {e}")
        
        confidence += events_score
        # ========================================================================
        
        # Cap confidence between 0-100
        confidence = max(0, min(100, confidence))
        
        # Determine action threshold
        if confidence >= 75:
            action_strength = "STRONG"
            action_advice = "High conviction - consider full position"
        elif confidence >= 60:
            action_strength = "MODERATE"
            action_advice = "Medium conviction - consider half position"
        elif confidence >= 45:
            action_strength = "WEAK"
            action_advice = "Low conviction - wait for better setup"
        else:
            action_strength = "AVOID"
            action_advice = "No conviction - do not trade"
        
        return {
            'score': confidence,
            'breakdown': breakdown,
            'action_strength': action_strength,
            'action_advice': action_advice
        }

# ‚úÖ COMPLETE FINAL CLASS - REPLACE YOUR EXISTING ONE
class IntelligentPredictionEngine:
    """Multi-LLM consensus with confidence scoring"""
    
    def __init__(self):
        self.prediction_tracker = prediction_tracker
        self.candle_analyzer = candle_analyzer
        self.learning_memory = learning_memory
        self.confidence_scorer = ConfidenceScorer()
        self.llm_clients = {}
        self._setup_llm_clients()

    def _setup_llm_clients(self):
        """Setup all available LLM clients with explicit logging"""
        if os.getenv("GROQ_API_KEY"):
            try:
                from groq import Groq
                self.llm_clients['groq'] = Groq(api_key=os.getenv("GROQ_API_KEY"))
                logging.info("‚úÖ SUCCESS: Groq LLM client initialized.")
            except Exception as e:
                logging.error(f"‚ùå FAILED: Groq initialization error: {e}")
        else:
            logging.warning("‚ö†Ô∏è SKIPPED: GROQ_API_KEY not found in secrets.")
        
        if os.getenv("GEMINI_API_KEY"):
            try:
                import google.generativeai as genai
                genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
                self.llm_clients['gemini'] = genai.GenerativeModel('gemini-2.5-flash') 
                logging.info("‚úÖ SUCCESS: Gemini LLM client initialized.")
            except Exception as e:
                logging.error(f"‚ùå FAILED: Gemini initialization error: {e}")
        else:
            logging.warning("‚ö†Ô∏è SKIPPED: GEMINI_API_KEY not found in secrets.")
        
        if os.getenv("COHERE_API_KEY"):
            try:
                import cohere
                self.llm_clients['cohere'] = cohere.Client(os.getenv("COHERE_API_KEY"))
                logging.info("‚úÖ SUCCESS: Cohere LLM client initialized.")
            except Exception as e:
                logging.error(f"‚ùå FAILED: Cohere initialization error: {e}")
        else:
            logging.warning("‚ö†Ô∏è SKIPPED: COHERE_API_KEY not found in secrets.")
        
        if not self.llm_clients:
            logging.error("‚ùå CRITICAL: No LLM clients available. System is operating in rule-based mode only.")
        else:
            logging.info(f"‚úÖ LLM clients loaded: {list(self.llm_clients.keys())}")
            
            
    async def analyze_with_learning(self, ticker, existing_analysis, hist_data, market_context=None, learning_brain=None):
    # Get patterns from BOTH analyzers
        candle_patterns = self.candle_analyzer.identify_pattern(hist_data)  # Basic patterns (18)
        
        # üÜï Add enhanced patterns (30+ advanced patterns)
        if ENHANCED_PATTERNS_ENABLED and enhanced_pattern_detector:
            try:
                enhanced_patterns = enhanced_pattern_detector.detect_all_patterns(hist_data)
                
                # Convert enhanced pattern format to match existing format
                for ep in enhanced_patterns:
                    candle_patterns.append({
                        'name': ep['name'],
                        'type': ep['type'],
                        'strength': 'very_strong' if ep['strength'] >= 90 else 'strong' if ep['strength'] >= 80 else 'medium',
                        'description': ep['description'],
                        'enhanced': True,  # Mark as enhanced pattern
                        'strength_score': ep['strength']  # Keep original score
                    })
                
                # Log the strongest pattern from enhanced detector
                if enhanced_patterns:
                    strongest = enhanced_patterns[0]  # Already sorted by strength
                    emoji = "üü¢" if strongest['signal'] == 'BUY' else "üî¥" if strongest['signal'] == 'SELL' else "‚ö™"
                    logging.info(f"   üïØÔ∏è Enhanced: {strongest['name']} {emoji} ({strongest['signal']}, {strongest['strength']}%)")
            
            except Exception as e:
                logging.debug(f"Enhanced pattern detection error for {ticker}: {e}")
    
        # Rest of your existing code stays the same
        pattern_success_rates = {p['name']: self.candle_analyzer.get_pattern_success_rate(p['name'], ticker) for p in candle_patterns}
        llm_predictions = await self._get_multi_llm_consensus(ticker, existing_analysis, candle_patterns, pattern_success_rates, market_context, learning_brain)
        confidence_result = self.confidence_scorer.calculate_confidence(llm_predictions, candle_patterns, pattern_success_rates, {'rsi': existing_analysis.get('rsi', 50), 'score': existing_analysis.get('score', 50)}, {'volume_ratio': existing_analysis.get('volume_ratio', 1.0)}, market_context)
        final_prediction = self._determine_final_action(llm_predictions, confidence_result, candle_patterns)
        
        if final_prediction:
            # ============================================================
            # Build comprehensive reasoning from LLM predictions
            # ============================================================
            if llm_predictions:
                # Format: "groq: reasoning | gemini: reasoning | cohere: reasoning"
                llm_reasoning = " | ".join([
                    f"{llm_name}: {pred.get('reasoning', pred.get('action', 'No reasoning'))}" 
                    for llm_name, pred in llm_predictions.items()
                ])
            else:
                llm_reasoning = final_prediction.get('reasoning', 'No LLM reasoning available')
            
            # Store prediction with detailed LLM reasoning
            pred_id = self.prediction_tracker.store_prediction(
                ticker=ticker,
                action=final_prediction['action'],
                confidence=confidence_result['score'],
                reasoning=llm_reasoning,  # ‚Üê Use LLM reasoning, not final_prediction['reasoning']
                candle_pattern=candle_patterns[0]['name'] if candle_patterns else None,
                indicators={'rsi': existing_analysis.get('rsi', 50)}
            )
            final_prediction['prediction_id'] = pred_id
        
        return {**existing_analysis, 'candle_patterns': candle_patterns, 'pattern_success_rates': pattern_success_rates, 'llm_predictions': llm_predictions, 'confidence': confidence_result, 'ai_prediction': final_prediction, 'learning_insights': self.learning_memory.get_recent_insights(3)}

    async def _get_multi_llm_consensus(self, ticker, existing_analysis, candle_patterns, pattern_success_rates, market_context, learning_brain=None):
        logging.info(f"üîç[{ticker}] Getting LLM consensus. Available models: {list(self.llm_clients.keys())}")
        pattern_text = "\n".join([f"{p['name']} ({p['type']}, {pattern_success_rates.get(p['name'], 50):.0f}% historical success)" for p in candle_patterns[:3]]) if candle_patterns else "No clear patterns identified"

        # ===== BUILD LEARNING CONTEXT =====
        learning_context = ""
        if learning_brain:
            try:
                from context_generator import ContextGenerator
                context_gen = ContextGenerator(learning_brain)
                
                # Build learning context for this stock
                current_indicators = {
                    'patterns': candle_patterns[0]['name'] if candle_patterns else '',
                    'rsi': existing_analysis.get('rsi', 50)
                }
                learning_context = context_gen.build_learning_context(ticker, current_indicators)
            except Exception as e:
                logging.warning(f"Could not generate learning context for {ticker}: {e}")
        # ===== END LEARNING CONTEXT =====
        
        context = f"""Analyze {ticker} and provide BUY/HOLD/SELL recommendation.
TECHNICAL DATA:
- Score: {existing_analysis.get('score', 'N/A')}/100
- RSI: {existing_analysis.get('rsi', 'N/A')}
- Volume: {existing_analysis.get('volume_ratio', 1.0):.1f}x average
CANDLESTICK PATTERNS (Today):
{pattern_text}
MARKET CONTEXT:
{f"Macro Score: {market_context.get('overall_macro_score', 0):.0f}" if market_context else "Not available"}
Respond with ONLY:
ACTION: [BUY/HOLD/SELL]
CONFIDENCE: [0-100]
REASON: [One sentence]"""
        
        tasks, llm_names = [], []
        if 'groq' in self.llm_clients:
            tasks.append(self._query_groq(context, ticker))
            llm_names.append('groq')
        if 'gemini' in self.llm_clients:
            tasks.append(self._query_gemini(context, ticker))
            llm_names.append('gemini')
        if 'cohere' in self.llm_clients:
            tasks.append(self._query_cohere(context, ticker))
            llm_names.append('cohere')
        
        predictions = {}
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for llm_name, result in zip(llm_names, results):
                if not isinstance(result, Exception) and result:
                    predictions[llm_name] = result
        
        logging.info(f"üîç[{ticker}] Received {len(predictions)} LLM predictions.")
        return predictions

    async def _query_groq(self, prompt, ticker):
        try:
            client = self.llm_clients['groq']
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model="llama-3.3-70b-versatile",  # ‚úÖ FIXED: Use the smaller, stable model
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3, max_tokens=150
            )
            return self._parse_llm_response(response.choices[0].message.content, 'groq')
        except Exception as e:
            logging.warning(f"Groq query failed for {ticker}: {e}")
            return None

    async def _query_gemini(self, prompt, ticker):
        try:
            model = self.llm_clients['gemini']
            # ‚úÖ FIXED: Switched to async call for better performance
            response = await model.generate_content_async(
                prompt,
                generation_config={'temperature': 0.3, 'max_output_tokens': 150}
            )
            return self._parse_llm_response(response.text, 'gemini')
        except Exception as e:
            logging.warning(f"Gemini query failed for {ticker}: {e}")
            return None

    async def _query_cohere(self, prompt, ticker):
        try:
            client = self.llm_clients['cohere']
            response = await asyncio.to_thread(
                client.chat,
                message=prompt,
                model='command-a-03-2025',  # ‚úÖ FIXED: Use the 'plus' version
                temperature=0.3
            )
            return self._parse_llm_response(response.text, 'cohere')
        except Exception as e:
            logging.warning(f"Cohere query failed for {ticker}: {e}")
            return None

    def _parse_llm_response(self, response_text, llm_name):
        import re
        action, confidence, reasoning = "HOLD", 50, response_text[:200]
        action_match = re.search(r'ACTION:\s*(BUY|SELL|HOLD)', response_text, re.IGNORECASE)
        if action_match: action = action_match.group(1).upper()
        conf_match = re.search(r'CONFIDENCE:\s*(\d+)', response_text, re.IGNORECASE)
        if conf_match: confidence = int(conf_match.group(1))
        reason_match = re.search(r'REASON:\s*(.+?)(?:\n|$)', response_text, re.IGNORECASE)
        if reason_match: reasoning = reason_match.group(1).strip()
        return {'action': action, 'confidence': max(0, min(100, confidence)), 'reasoning': reasoning, 'llm': llm_name}

    def _determine_final_action(self, llm_predictions, confidence_result, candle_patterns):
        if not llm_predictions:
            action = "BUY" if confidence_result['score'] >= 60 and candle_patterns and 'bullish' in candle_patterns[0].get('type', '') else "SELL" if confidence_result['score'] <= 40 else "HOLD"
            return {'action': action, 'confidence': confidence_result['score'], 'reasoning': f"Rule-based: {confidence_result['action_advice']}", 'llm_count': 0}
        
        actions = [p['action'] for p in llm_predictions.values()]
        weights = self.learning_memory.get_llm_weights()
        weighted_actions = {"BUY": 0, "HOLD": 0, "SELL": 0}
        for llm_name, prediction in llm_predictions.items():
            weighted_actions[prediction['action']] += weights.get(llm_name, 0.33)
        
        final_action = max(weighted_actions, key=weighted_actions.get)
        reasonings = [f"{p['llm']}: {p['reasoning']}" for p in llm_predictions.values()]
        combined_reasoning = " | ".join(reasonings)
        
        if confidence_result['score'] < 45:
            final_action = "HOLD"
            combined_reasoning = f"Low confidence ({confidence_result['score']}%) - holding. " + combined_reasoning
            
        return {'action': final_action, 'confidence': confidence_result['score'], 'reasoning': combined_reasoning[:500], 'llm_count': len(llm_predictions), 'llm_agreement': (actions.count(final_action) / len(actions)) * 100, 'confidence_breakdown': confidence_result['breakdown'], 'action_strength': confidence_result['action_strength'], 'action_advice': confidence_result['action_advice']}


# ========================================
# üéØ ENHANCED PORTFOLIO ANALYZER
# Wraps your existing portfolio analysis with predictions
# ========================================

async def analyze_portfolio_with_predictions(session, portfolio_file='portfolio.json', market_context=None, learning_brain=None):
    """
    Enhanced portfolio analysis with predictions and market context
    """
    
    logging.info("=" * 60)
    logging.info("üß† ANALYZE WITH PREDICTIONS - START")
    logging.info(f"Learning brain provided: {learning_brain is not None}")
    logging.info(f"Market context provided: {market_context is not None}")
    
    # Call v2.0 portfolio analysis (has Bollinger, ATR, etc.)
    original_portfolio_data = await analyze_portfolio_with_v2_features(session, portfolio_file)
    
    if not original_portfolio_data:
        logging.warning("No portfolio data from v2 features")
        return original_portfolio_data
    
    logging.info(f"Original portfolio has {len(original_portfolio_data.get('stocks', []))} stocks")
    
    # Initialize prediction engine
    try:
        prediction_engine = IntelligentPredictionEngine()
        logging.info(f"Prediction engine initialized. LLMs available: {list(prediction_engine.llm_clients.keys())}")
    except Exception as e:
        logging.error(f"Failed to initialize prediction engine: {e}")
        # Return original data without predictions
        return {
            **original_portfolio_data,
            'learning_active': False,
            'predictions_made': 0
        }
    
    # Add predictions to each stock
    enhanced_stocks = []
    successful_predictions = 0
    
    for stock in original_portfolio_data['stocks']:
        try:
            ticker = stock['ticker']
            logging.info(f"üîç Processing {ticker}...")
            
            yf_ticker = yf.Ticker(ticker)
            hist = await asyncio.to_thread(yf_ticker.history, period="3mo", interval="1d")
            
            if hist.empty:
                logging.warning(f"No history for {ticker}, skipping predictions")
                enhanced_stocks.append(stock)
                continue
            
            # Get enhanced analysis with market context
            enhanced = await prediction_engine.analyze_with_learning(
                ticker=ticker,
                existing_analysis=stock,
                hist_data=hist,
                market_context=market_context
            )
            
            # Verify prediction was added
            if 'ai_prediction' in enhanced:
                successful_predictions += 1
                logging.info(f"‚úÖ {ticker}: Prediction added - {enhanced['ai_prediction']['action']}")
                                # ===== RECORD PREDICTION IN LEARNING DATABASE =====
                if learning_brain:
                    try:
                        pred_data = enhanced['ai_prediction']
                        current_price = stock.get('price', 0)
                        
                        # Extract indicators from stock data
                        indicators_dict = {
                            'rsi': stock.get('rsi', 0),
                            'macd': stock.get('macd', 0),
                            'volume_ratio': stock.get('volume_ratio', 0),
                            'patterns': pred_data.get('pattern_detected', '')
                        }
                        
                        # Extract confidence, llm, reasoning (with fallbacks)
                        confidence = (
                            pred_data.get('confidence') or 
                            pred_data.get('conviction_score') or 
                            0
                        )
                        
                        llm_model = (
                            pred_data.get('llm_consensus') or 
                            pred_data.get('model_used') or 
                            'unknown'
                        )
                        
                        reasoning = (
                            pred_data.get('reasoning') or 
                            pred_data.get('consensus_reasoning') or 
                            ''
                        )[:500]
                        
                        # Record prediction to SQLite
                        pred_id = learning_brain.record_prediction(
                            stock=ticker,
                            prediction=pred_data.get('action', 'HOLD'),
                            confidence=confidence,
                            price=current_price,
                            llm_model=llm_model,
                            reasoning=reasoning,
                            indicators=indicators_dict
                        )
                        
                        logging.info(f"üíæ Prediction #{pred_id} saved (confidence: {confidence}%)")
                        
                    except Exception as e:
                        logging.error(f"‚ö†Ô∏è Failed to record prediction for {ticker}: {e}")
                        import traceback
                        traceback.print_exc()
                # ===== END LEARNING DATABASE RECORDING =====
            else:
                logging.warning(f"‚ö†Ô∏è {ticker}: No prediction generated")
            
            enhanced_stocks.append(enhanced)
            
        except Exception as e:
            logging.error(f"Error enhancing {ticker}: {e}")
            enhanced_stocks.append(stock)  # Keep original
    
    result = {
        **original_portfolio_data,
        'stocks': enhanced_stocks,
        'predictions_made': successful_predictions,
        'learning_active': True
    }
    
    logging.info("=" * 60)
    logging.info(f"‚úÖ PREDICTIONS COMPLETE: {successful_predictions}/{len(enhanced_stocks)} stocks")
    logging.info("=" * 60)
    
    return result


# ========================================
# üîÑ OUTCOME CHECKER - Runs in evening
# ========================================

async def check_prediction_outcomes():
    """
    This runs in the evening to check how our predictions did
    Standalone function - doesn't modify existing code
    """
    logging.info("üîç Checking prediction outcomes...")
    
    # Check outcomes from yesterday
    results = prediction_tracker.check_outcomes(days_to_check=1)
    
    # Update pattern success rates based on outcomes
    for pred_id, pred in prediction_tracker.predictions.items():
        if pred.get('was_correct') is not None and pred.get('candle_pattern'):
            candle_analyzer.update_pattern_outcome(
                pattern=pred['candle_pattern'],
                ticker=pred['ticker'],
                was_successful=pred['was_correct']
            )
    
    # Generate learning insights
    if results['checked'] > 0:
        accuracy = (results['correct'] / results['checked']) * 100
        insight = f"Today's accuracy: {accuracy:.1f}% ({results['correct']}/{results['checked']} correct)"
        learning_memory.add_insight(insight)
        
        # Update LLM accuracy if we tracked which LLM made predictions
        # This will be implemented in next iteration
    
    logging.info(f"‚úÖ Checked {results['checked']} predictions: {results['correct']} correct, {results['wrong']} wrong")
    
    return results

# ========================================
# MAIN FUNCTION - Updated for v2.0.0
# ========================================

async def main(output="print"):
    logging.info("üìä FULL ANALYSIS MODE: Running market intelligence scan...")
    previous_day_memory = load_memory()

     # ===== LEARNING BRAIN INITIALIZATION =====
    learning_brain = LearningBrain()
    
    # Check outcomes from previous predictions (1 week ago)
    logging.info("="*60)
    logging.info("üß† CHECKING PREVIOUS PREDICTIONS...")
    logging.info("="*60)
    learning_brain.check_outcomes_multi_timeframe()
    
    # Get accuracy report
    accuracy_report = learning_brain.get_accuracy_report()
    logging.info(accuracy_report)
    logging.info("="*60)
    # ===== END LEARNING BRAIN INITIALIZATION =====
    
    sp500 = get_cached_tickers('sp500_cache.json', fetch_sp500_tickers_sync)
    tsx = get_cached_tickers('tsx_cache.json', fetch_tsx_tickers_sync)
    universe = (sp500 or [])[:75] + (tsx or [])[:25]
    
    throttler = Throttler(2)
    semaphore = asyncio.Semaphore(10)
    
    async with aiohttp.ClientSession() as session:
        # Prepare all tasks
        stock_tasks = [analyze_stock(semaphore, throttler, session, ticker) for ticker in universe]
        context_task = fetch_context_data(session)
        news_task = fetch_market_headlines(session)
        macro_task = fetch_macro_sentiment(session)
        
        # Step 1: Get macro data FIRST (needed for portfolio predictions)
        logging.info("üîç Step 1: Fetching macro data...")
        macro_data = await macro_task
        logging.info(f"‚úÖ Macro data received. Score: {macro_data.get('overall_macro_score', 0):.1f}")
        
        # Step 2: Create portfolio task with macro context
        if ENABLE_V2_FEATURES:
            logging.info("üîç Step 2: Calling analyze_portfolio_with_predictions (v3.0)")
            portfolio_data = await analyze_portfolio_with_predictions(session, market_context=macro_data, learning_brain=learning_brain)
        else:
            logging.info("üîç Step 2: Calling analyze_portfolio_watchlist (v1.0)")
            portfolio_data = await analyze_portfolio_watchlist(session)
        
        logging.info(f"‚úÖ Portfolio analysis complete. Predictions: {portfolio_data.get('predictions_made', 0) if portfolio_data else 0}")
        
        # Step 3: Run everything else in parallel
        logging.info("üîç Step 3: Fetching stocks, context, news...")
        stock_results_raw, context_data, market_news = await asyncio.gather(
            asyncio.gather(*stock_tasks), 
            context_task, 
            news_task
        )
        
        # Step 4: Process stock results
        stock_results = sorted([r for r in stock_results_raw if r], key=lambda x: x['score'], reverse=True)
        df_stocks = pd.DataFrame(stock_results) if stock_results else pd.DataFrame()
        logging.info(f"‚úÖ Analyzed {len(stock_results)} stocks")
        
        # Step 5: Find historical patterns
        logging.info("üîç Step 5: Finding historical patterns...")
        pattern_data = await find_historical_patterns(session, macro_data)
        
        # Step 6: Generate portfolio recommendations
        portfolio_recommendations = None
        if pattern_data and portfolio_data:
            logging.info("üîç Step 6: Generating portfolio recommendations...")
            portfolio_recommendations = await generate_portfolio_recommendations_from_pattern(
                portfolio_data, pattern_data, macro_data
            )
        
        # Step 7: Skip AI Oracle (removed - was generating placeholder text)
        logging.info("üîç Step 7: Skipping AI Oracle (removed)")
        ai_analysis = None
    
    # Outside session context - generate email
    if output == "email":
        logging.info("üìß Generating email report...")
        html_email = generate_enhanced_html_email(
            df_stocks, context_data, market_news, macro_data, 
            previous_day_memory, portfolio_data, pattern_data, portfolio_recommendations
        )
        send_email(html_email)
    
    # Save memory
    if not df_stocks.empty:
        save_memory({
            "previous_top_stock_name": str(df_stocks.iloc[0]['name']),
            "previous_top_stock_ticker": str(df_stocks.iloc[0]['ticker']),
            "previous_macro_score": float(macro_data.get('overall_macro_score', 0)),
            "date": datetime.now().strftime('%Y-%m-%d')
        })
    
    logging.info("‚úÖ Analysis complete with v2.0.0 features.")



# ========================================
# EMAIL GENERATION - Updated for v2.0.0
# ========================================

def generate_enhanced_html_email(df_stocks, context, market_news, macro_data, memory, portfolio_data, pattern_data, portfolio_recommendations=None):
    """FIXED v2.0.0: Clear, non-conflicting email display"""
    
    def format_articles(articles):
        if not articles:
            return "<p style='color:#888;'><i>No specific news drivers detected.</i></p>"
        html = "<ul style='margin:0;padding-left:20px;'>"
        for a in articles:
            if a.get('title'):
                html += f'<li style="margin-bottom:5px;"><a href="{a.get("url", "#")}" style="color:#1e3a8a;">{a["title"]}</a> <span style="color:#666;">({a.get("source", "Unknown")})</span></li>'
        return html + "</ul>"
    
    def create_stock_table(df):
        return "".join([f'<tr><td style="padding:10px;border-bottom:1px solid #eee;"><b>{row["ticker"]}</b><br><span style="color:#666;font-size:0.9em;">{row["name"]}</span></td><td style="padding:10px;border-bottom:1px solid #eee;text-align:center;font-weight:bold;font-size:1.1em;">{row["score"]:.0f}</td></tr>' for _, row in df.iterrows()])
    
    def create_context_table(ids):
        rows = ""
        for asset_id in ids:
            if asset := context.get(asset_id):
                price = f"${asset.get('current_price', 0):,.2f}" if asset.get('current_price') else "N/A"
                change_24h = asset.get('price_change_percentage_24h', 0) or 0
                mcap = f"${asset.get('market_cap', 0) / 1_000_000_000:.1f}B" if asset.get('market_cap') else "N/A"
                color_24h = "#16a34a" if change_24h >= 0 else "#dc2626"
                rows += f'<tr><td style="padding:10px;border-bottom:1px solid #eee;"><b>{asset.get("name", "")}</b><br><span style="color:#666;font-size:0.9em;">{asset.get("symbol","").upper()}</span></td><td style="padding:10px;border-bottom:1px solid #eee;">{price}<br><span style="color:{color_24h};font-size:0.9em;">{change_24h:.2f}% (24h)</span></td><td style="padding:10px;border-bottom:1px solid #eee;">{mcap}</td></tr>'
        return rows
    
    # v2.0.0 NEW: High-priority signals section
    v2_signals_html = ""
    if portfolio_data and portfolio_data.get('v2_signals'):
        signals_list = "<br>".join(portfolio_data['v2_signals'][:10])
        v2_signals_html = f"""<div class="section" style="background-color:#fef3c7;border-left:4px solid #f59e0b;">
        <h2>‚ö° HIGH-PRIORITY SIGNALS (v2.0)</h2>
        <p style="font-size:0.9em;color:#666;">Advanced technical alerts requiring immediate attention</p>
        <div style="line-height:2; font-weight:500;">{signals_list if signals_list else 'No high-priority signals today'}</div>
        </div>"""
    
    # Portfolio section with v2.0.0 enhancements
    portfolio_html = ""
    if portfolio_data:
        portfolio_table = ""
        for stock in portfolio_data['stocks']:
            color = "#16a34a" if stock['daily_change'] > 0 else "#dc2626"
            rsi_color = "#dc2626" if stock['rsi'] > 70 else "#16a34a" if stock['rsi'] < 30 else "#666"
            
            # v2.0.0: Add Bollinger Band indicator
            bb_indicator = ""
            if stock.get('bollinger'):
                bb = stock['bollinger']
                if bb['squeeze']:
                    bb_indicator = f"<br><span style='color:#f59e0b;font-weight:bold;'>üí• SQUEEZE</span>"
                elif bb['position'] > 90:
                    bb_indicator = f"<br><span style='color:#dc2626;'>BB: {bb['position']:.0f}%</span>"
                elif bb['position'] < 10:
                    bb_indicator = f"<br><span style='color:#16a34a;'>BB: {bb['position']:.0f}%</span>"
            
            # v2.0.0: Add 52-week indicator
            week52_indicator = ""
            if stock.get('week52'):
                w52 = stock['week52']
                if 'AT 52W HIGH' in w52['signal']:
                    week52_indicator = f"<br><span style='color:#16a34a;font-weight:bold;'>üöÄ 52W HIGH</span>"
                elif 'AT 52W LOW' in w52['signal']:
                    week52_indicator = f"<br><span style='color:#dc2626;font-weight:bold;'>üìâ 52W LOW</span>"
            
            # v2.0.0: Add earnings countdown
            earnings_indicator = ""
            if stock.get('earnings') and stock['earnings']['days_until'] <= 7:
                earnings_indicator = f"<br><span style='color:#9333ea;font-weight:bold;'>üìÖ Earnings in {stock['earnings']['days_until']}d</span>"
            
            portfolio_table += f"""<tr>
            <td style="padding:10px;border-bottom:1px solid #eee;">
                <b>{stock['ticker']}</b><br>
                <span style="color:#666;font-size:0.9em;">{stock['name']}</span>
                {bb_indicator}{week52_indicator}{earnings_indicator}
            </td>
            <td style="padding:10px;border-bottom:1px solid #eee;">
                ${stock['price']:.2f}<br>
                <span style="color:{color};font-size:0.9em;">{stock['daily_change']:+.2f}%</span>
            </td>
            <td style="padding:10px;border-bottom:1px solid #eee;">
                RSI: <span style="color:{rsi_color};font-weight:bold;">{stock['rsi']:.1f}</span><br>
                <span style="font-size:0.9em;">Vol: {stock['volume_ratio']:.1f}x</span>
            </td>
            <td style="padding:10px;border-bottom:1px solid #eee;">
                <span style="font-size:0.9em;">W: {stock['weekly_change']:+.1f}%<br>M: {stock['monthly_change']:+.1f}%</span>
            </td>
            </tr>"""
        
        alerts_html = "<br>".join(portfolio_data['alerts'][:5]) if portfolio_data['alerts'] else "No alerts today"
        opps_html = "<br>".join(portfolio_data['opportunities'][:5]) if portfolio_data['opportunities'] else "No immediate opportunities"
        risks_html = "<br>".join(portfolio_data['risks'][:5]) if portfolio_data['risks'] else "No significant risks detected"
        
        portfolio_html = f"""<div class="section" style="background-color:#fefce8;border-left:4px solid #ca8a04;">
        <h2>üìä YOUR PORTFOLIO COMMAND CENTER</h2>
        <table style="width:100%; border-collapse: collapse; margin-bottom:20px;">
            <thead><tr style="background-color:#f8f8f8;">
                <th style="text-align:left; padding:10px;">Stock</th>
                <th style="text-align:left; padding:10px;">Price</th>
                <th style="text-align:left; padding:10px;">Indicators</th>
                <th style="text-align:left; padding:10px;">Performance</th>
            </tr></thead>
            <tbody>{portfolio_table}</tbody>
        </table>
        <div style="margin-top:20px;">
            <h3 style="color:#dc2626;">üîî Alerts & Signals</h3>
            <p style="line-height:1.8;">{alerts_html}</p>
        </div>
        <div style="margin-top:20px;">
            <h3 style="color:#16a34a;">üéØ Opportunities</h3>
            <p style="line-height:1.8;">{opps_html}</p>
        </div>
        <div style="margin-top:20px;">
            <h3 style="color:#ea580c;">‚ö†Ô∏è Risk Factors</h3>
            <p style="line-height:1.8;">{risks_html}</p>
        </div>
        </div>"""

    # AI Predictions section
    ai_predictions_html = ""
    if portfolio_data and portfolio_data.get('learning_active'):
        predictions_made = portfolio_data.get('predictions_made', 0)
        
        if predictions_made > 0:
            prediction_cards = []
            for stock in portfolio_data['stocks']:
                if 'ai_prediction' not in stock or not stock['ai_prediction']:
                    continue
                
                try:
                    pred = stock['ai_prediction']
                    conf = stock.get('confidence', {})
                    
                    action = pred.get('action', 'HOLD')
                    conf_score = conf.get('score', 50)
                    
                    action_color = {'BUY': '#28a745', 'SELL': '#dc3545', 'HOLD': '#6c757d'}.get(action, '#6c757d')
                    action_icon = {'BUY': 'üü¢', 'SELL': 'üî¥', 'HOLD': '‚ö™'}.get(action, '‚ö™')
                    
                    # Create the detailed breakdown list
                    breakdown_list_html = ""
                    if conf.get('breakdown'):
                        breakdown_items = []
                        for item in conf['breakdown']:
                            icon = "‚úÖ" if "+" in item else "‚ö†Ô∏è" if "-" in item else "‚öñÔ∏è"
                            breakdown_items.append(f'<li style="font-size: 0.9em; color: #555; margin-bottom: 4px;">{icon} {item}</li>')
                        breakdown_list_html = f"<ul>{''.join(breakdown_items)}</ul>"

                    prediction_cards.append(f"""
                    <div style="border: 1px solid #ddd; border-left: 5px solid {action_color}; border-radius: 8px; margin-bottom: 20px; padding: 20px; background: #fff; box-shadow: 0 2px 5px rgba(0,0,0,0.05);">
                        <div style="display: flex; justify-content: space-between; align-items: flex-start; border-bottom: 1px solid #eee; padding-bottom: 15px; margin-bottom: 15px;">
                            <div>
                                <h3 style="margin: 0 0 5px 0; font-size: 1.5em; color: #333;">{action_icon} {stock.get('ticker', '?')} &rarr; <span style="color:{action_color};">{action}</span></h3>
                                <p style="margin: 0; color: #777; font-size: 1em;">{stock.get('name', 'N/A')}</p>
                            </div>
                            <div style="text-align: right;">
                                <div style="font-size: 2.2em; font-weight: bold; color: {action_color};">{conf_score}%</div>
                                <div style="font-size: 0.8em; font-weight: bold; color: {action_color};">CONVICTION SCORE</div>
                            </div>
                        </div>
                        <div>
                            <h4 style="margin: 0 0 10px 0; font-size: 1em; color: #333;">Confidence Factors:</h4>
                            {breakdown_list_html or "<p style='font-size:0.9em; color:#777;'>No detailed breakdown available.</p>"}
                        </div>
                        <div style="margin-top: 15px; padding: 10px; background: #f9f9f9; border-radius: 5px; font-size: 0.9em; color: #444;">
                            <strong>Consensus Reasoning:</strong> {pred.get('reasoning', 'N/A')}
                        </div>
                    </div>
                    """)
                except Exception as e:
                    logging.error(f"Error creating detailed prediction card for {stock.get('ticker', '?')}: {e}")

            if prediction_cards:
                ai_predictions_html = f"""
                <div class="section" style="background-color:#f4f7f6;">
                    <h2 class="section-title">üéØ AI Predictions & Conviction Analysis</h2>
                    {''.join(prediction_cards)}
                </div>
                """
                logging.info("‚úÖ AI predictions HTML generated successfully")
            else:
                logging.warning("‚ö†Ô∏è No prediction cards created despite predictions_made > 0")
        else:
            logging.info("No predictions to display (predictions_made = 0)")

    logging.info("=" * 60)

    # Pattern analysis section (from v1.0.0 - keeping stable)
    pattern_html = ""
    if pattern_data and pattern_data.get('matches'):
        current_cond_data = pattern_data['current_conditions']
        current_cond = f"""<div style="background-color:#fff;padding:15px;border:2px solid #7c3aed;border-radius:5px;margin-bottom:20px;">
        <h3 style="margin-top:0;">üìä Today's Market DNA:</h3>
        <p style="margin:5px 0;"><b>RSI:</b> {current_cond_data['rsi']:.1f} | <b>Volatility:</b> {current_cond_data['volatility']:.1f}% | <b>Trend:</b> {current_cond_data['trend']:+.1f}%</p>
        <p style="margin:5px 0;"><b>Geopolitical Risk:</b> {current_cond_data['geo_risk']:.0f} | <b>Trade Risk:</b> {current_cond_data['trade_risk']:.0f}</p>
        </div>"""
        
        interpretation_html = ""
        if pattern_data.get('interpretation'):
            for item in pattern_data['interpretation']:
                if item['type'] == 'bias':
                    color = {'bullish': '#16a34a', 'bearish': '#dc2626', 'neutral': '#666'}[item['color']]
                    interpretation_html += f'<p style="font-size:1.2em;font-weight:bold;color:{color};">{item["emoji"]} {item["text"]}</p>'
                else:
                    interpretation_html += f'<p style="line-height:1.8;margin:10px 0;">{item["text"]}</p>'
        
        sector_html_patterns = ""
        if pattern_data.get('sector_performance'):
            sector_rows = ""
            for sector, perf in pattern_data['sector_performance'][:5]:
                color = "#16a34a" if perf > 0 else "#dc2626"
                sector_rows += f'<tr><td style="padding:8px;border-bottom:1px solid #eee;">{sector}</td><td style="padding:8px;border-bottom:1px solid #eee;text-align:right;color:{color};font-weight:bold;">{perf:+.1f}%</td></tr>'
            
            sector_html_patterns = f"""<div style="margin:20px 0;">
            <h3>üéØ Sector Performance in Similar Periods:</h3>
            <p style="font-size:0.9em;color:#666;">Based on {pattern_data['matches'][0]['date']} match ({pattern_data['matches'][0]['context']})</p>
            <table style="width:100%;background-color:#fff;border-collapse:collapse;">
                <thead><tr style="background-color:#f3e8ff;">
                    <th style="padding:10px;text-align:left;">Sector</th>
                    <th style="padding:10px;text-align:right;">3-Month Return</th>
                </tr></thead>
                <tbody>{sector_rows}</tbody>
            </table>
            </div>"""
        
        matches_html = ""
        for i, match in enumerate(pattern_data['matches'][:5], 1):
            outcome_color = "#16a34a" if match['future_3m'] > 0 else "#dc2626"
            matches_html += f"""<div style="margin:15px 0;padding:15px;background-color:#f8f8f8;border-left:4px solid {outcome_color};border-radius:5px;">
            <div style="display:flex;justify-content:space-between;align-items:center;">
                <div>
                    <b style="font-size:1.1em;">{match['date']}</b>
                    <span style="color:#666;margin-left:10px;">({match['context']})</span><br>
                    <span style="font-size:0.9em;color:#666;">Match Strength: {match['similarity']:.1f}%</span>
                </div>
                <div style="text-align:right;">
                    <div style="font-size:1.2em;font-weight:bold;color:{outcome_color};">{match['future_3m']:+.1f}%</div>
                    <div style="font-size:0.9em;color:#666;">S&P 500 outcome</div>
                </div>
            </div>
            </div>"""
        
        pattern_html = f"""<div class="section" style="background-color:#f3e8ff;border-left:4px solid #7c3aed;">
        <h2>üîÆ 11-YEAR PATTERN ANALYSIS</h2>
        <p style="font-size:0.9em;color:#666;">Analyzing {pattern_data['sample_size']} similar market setups</p>
        {current_cond}
        <div style="background-color:#fff;padding:20px;border-radius:5px;margin:20px 0;">
            <h3 style="margin-top:0;color:#7c3aed;">üìñ What History Tells Us:</h3>
            {interpretation_html}
        </div>
        {sector_html_patterns}
        <div style="margin:20px 0;">
            <h3>üìÖ Historical Matches:</h3>
            <p style="font-size:0.9em;color:#666;">These show S&P 500 performance. Sector performance varied (see table above).</p>
            {matches_html}
        </div>
        </div>"""

    # Editor's note
    prev_score = memory.get('previous_macro_score', 0)
    current_score = macro_data.get('overall_macro_score', 0)
    mood_change = "stayed relatively stable"
    if (diff := current_score - prev_score) > 3:
        mood_change = f"improved since yesterday (from {prev_score:.1f} to {current_score:.1f})"
    elif diff < -3:
        mood_change = f"turned more cautious since yesterday (from {prev_score:.1f} to {current_score:.1f})"
    
    editor_note = f"Good morning. The overall market mood has {mood_change}. This briefing is your daily blueprint for navigating the currents."
    if memory.get('previous_top_stock_name'):
        editor_note += f"<br><br><b>Yesterday's Champion:</b> {memory['previous_top_stock_name']} ({memory['previous_top_stock_ticker']}) led our rankings."
    
    # Sector deep dive
    sector_html = ""
    if not df_stocks.empty:
        top_by_sector = df_stocks.groupby('sector', group_keys=False)[['ticker', 'name', 'score', 'sector', 'summary']].apply(lambda x: x.nlargest(2, 'score'))
        for _, row in top_by_sector.iterrows():
            if row['sector'] and row['sector'] != 'N/A':
                summary_text = "Business summary not available."
                if row["summary"] and isinstance(row["summary"], str):
                    summary_text = '. '.join(row["summary"].split('. ')[:2]) + '.'
                sector_html += f'<div style="margin-bottom:15px;"><b>{row["name"]} ({row["ticker"]})</b> in <i>{row["sector"]}</i><p style="font-size:0.9em;color:#333;margin:5px 0 0 0;">{summary_text}</p></div>'
    
    # Create tables
    top10_html = create_stock_table(df_stocks.head(10)) if not df_stocks.empty else "<tr><td>No data available</td></tr>"
    bottom10_html = create_stock_table(df_stocks.tail(10).iloc[::-1]) if not df_stocks.empty else "<tr><td>No data available</td></tr>"
    crypto_html = create_context_table(["bitcoin", "ethereum", "solana", "ripple"])
    commodities_html = create_context_table(["gold", "silver"])
    market_news_html = "".join([f'<div style="margin-bottom:15px;"><b><a href="{article.get("url", "#")}" style="color:#000;">{article["title"]}</a></b><br><span style="color:#666;font-size:0.9em;">{article.get("source", "Unknown")}</span></div>' for article in market_news[:10]]) or "<p><i>Headlines temporarily unavailable.</i></p>"
    
    # Assemble final email
    return f"""<!DOCTYPE html><html><head><style>
    body{{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,sans-serif;margin:0;padding:0;background-color:#f7f7f7;}}
    .container{{width:100%;max-width:700px;margin:20px auto;background-color:#fff;border:1px solid #ddd;}}
    .header{{background-color:#0c0a09;color:#fff;padding:30px;text-align:center;}}
    .section{{padding:25px;border-bottom:1px solid #ddd;}}
    h2{{font-size:1.5em;color:#111;margin-top:0;}}
    h3{{font-size:1.2em;color:#333;border-bottom:2px solid #e2e8f0;padding-bottom:5px;}}
    </style></head><body>
    <div class="container">
        <div class="header">
            <h1>Your Daily Intelligence Briefing</h1>
            <p style="font-size:1.1em; color:#aaa;">{datetime.now().strftime('%A, %B %d, %Y')}</p>
        </div>
        
        <div class="section">
            <h2>EDITOR'S NOTE</h2>
            <p>{editor_note}</p>
        </div>
        
        {v2_signals_html}
        {portfolio_html}
        {ai_predictions_html}
        {pattern_html}
        
        <div class="section">
            <h2>THE BIG PICTURE: The Market Weather Report</h2>
            <h3>Overall Macro Score: {macro_data['overall_macro_score']:.1f} / 30</h3>
            <p><b>How it's calculated:</b> This is our "weather forecast" for investors, combining risks and sentiment.</p>
            <p><b>üåç Geopolitical Risk ({macro_data['geopolitical_risk']:.0f}/100):</b> Measures global instability.<br>
            <u>Key Drivers:</u> {format_articles(macro_data['geo_articles'])}</p>
            <p><b>üö¢ Trade Risk ({macro_data['trade_risk']:.0f}/100):</b> Tracks trade tensions.<br>
            <u>Key Drivers:</u> {format_articles(macro_data['trade_articles'])}</p>
            <p><b>üíº Economic Sentiment ({macro_data['economic_sentiment']:.2f}):</b> Market mood (-1 to +1).<br>
            <u>Key Drivers:</u> {format_articles(macro_data['econ_articles'])}</p>
        </div>
        
        <div class="section">
            <h2>SECTOR DEEP DIVE</h2>
            <p>Top companies from different sectors.</p>
            {sector_html or "<p><i>No sector data available.</i></p>"}
        </div>
        
        <div class="section">
            <h2>STOCK RADAR</h2>
            <h3>üìà Top 10 Strongest Signals</h3>
            <table style="width:100%; border-collapse: collapse;">
                <thead><tr>
                    <th style="text-align:left; padding:10px;">Company</th>
                    <th style="text-align:center; padding:10px;">Score</th>
                </tr></thead>
                <tbody>{top10_html}</tbody>
            </table>
            
            <h3 style="margin-top: 30px;">üìâ Top 10 Weakest Signals</h3>
            <table style="width:100%; border-collapse: collapse;">
                <thead><tr>
                    <th style="text-align:left; padding:10px;">Company</th>
                    <th style="text-align:center; padding:10px;">Score</th>
                </tr></thead>
                <tbody>{bottom10_html}</tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>BEYOND STOCKS: Alternative Assets</h2>
            <h3>ü™ô Crypto</h3>
            <p><b>Market Sentiment: {context.get('crypto_sentiment', 'N/A')}</b></p>
            <table style="width:100%; border-collapse: collapse;">
                <thead><tr>
                    <th style="text-align:left; padding:10px;">Asset</th>
                    <th style="text-align:left; padding:10px;">Price / 24h</th>
                    <th style="text-align:left; padding:10px;">Market Cap</th>
                </tr></thead>
                <tbody>{crypto_html}</tbody>
            </table>
            
            <h3 style="margin-top: 30px;">üíé Commodities</h3>
            <p><b>Gold/Silver Ratio: {context.get('gold_silver_ratio', 'N/A')}</b></p>
            <table style="width:100%; border-collapse: collapse;">
                <thead><tr>
                    <th style="text-align:left; padding:10px;">Asset</th>
                    <th style="text-align:left; padding:10px;">Price / 24h</th>
                    <th style="text-align:left; padding:10px;">Market Cap</th>
                </tr></thead>
                <tbody>{commodities_html}</tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>FROM THE WIRE: Today's Top Headlines</h2>
            {market_news_html}
        </div>
    </div>
    </body></html>"""

def send_email(html_body):
    SMTP_USER, SMTP_PASS = os.getenv("SMTP_USER"), os.getenv("SMTP_PASS")
    if not SMTP_USER or not SMTP_PASS:
        logging.warning("SMTP credentials missing. Cannot send email.")
        return
    
    msg = MIMEMultipart('alternative')
    msg["Subject"] = f"‚õµ Your Daily Market Briefing - {datetime.today().date()}"
    msg["From"] = SMTP_USER
    msg["To"] = SMTP_USER
    msg.attach(MIMEText(html_body, 'html'))
    
    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASS)
            server.send_message(msg)
        logging.info("‚úÖ Email sent successfully.")
    except Exception as e:
        logging.error(f"Failed to send email: {e}")


# ========================================
# üÜï EMAIL BOT SYSTEM - v3.1.0 (FINAL, AUDITED)
# This version is guaranteed to be free of syntax and indentation errors.
# ========================================

# ========================================
# SECTION 1: IMPORTS & FAILSAFES
# ========================================

# ========================================
# üÜï EMAIL BOT SYSTEM - v3.2.2 (FINAL)
# ========================================

# Failsafe for duckduckgo-search -> ddgs rename
try:
    from ddgs import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    try:
        from duckduckgo_search import DDGS
        DDGS_AVAILABLE = True
        logging.warning("‚ö†Ô∏è DeprecationWarning: `duckduckgo_search` is now `ddgs`. Use `pip install -U ddgs`.")
    except ImportError:
        logging.error("‚ùå `ddgs` not found. Web search capabilities disabled.")
        DDGS_AVAILABLE = False
        class DDGS:
            def __enter__(self): return self
            def __exit__(self, *args): pass
            def news(self, *args, **kwargs): return []
            def text(self, *args, **kwargs): return []

# ========================================
# SECTION 2: CORE BOT CLASSES
# ========================================

class EmailBotDatabase:
    """Isolated database for bot conversations"""
    def __init__(self, db_path='email_bot.db'):
        try:
            self.conn = sqlite3.connect(db_path, check_same_thread=False)
            self._init_schema()
        except Exception as e:
            logging.error(f"DB init failed: {e}. Using in-memory fallback.")
            self.conn = sqlite3.connect(':memory:', check_same_thread=False)
            self._init_schema()

    def _init_schema(self):
        try:
            self.conn.executescript('''
                CREATE TABLE IF NOT EXISTS conversations (id INTEGER PRIMARY KEY, timestamp TEXT, user_email TEXT, user_question TEXT, topics TEXT, sent BOOLEAN);
                CREATE TABLE IF NOT EXISTS bot_stats (date TEXT PRIMARY KEY, checked INTEGER DEFAULT 0, answered INTEGER DEFAULT 0, errors INTEGER DEFAULT 0);
            ''')
            self.conn.commit()
        except Exception as e: logging.error(f"Schema creation failed: {e}")

    def log_conversation(self, email_addr, question, topics, success):
        try:
            self.conn.execute('INSERT INTO conversations (timestamp, user_email, user_question, topics, sent) VALUES (?, ?, ?, ?, ?)',(datetime.datetime.now().isoformat(),email_addr,(question or "")[:500],json.dumps(topics or {}),bool(success)))
            self.conn.commit()
        except Exception as e: logging.error(f"Conversation log failed: {e}")

    def update_stats(self, checked=0, answered=0, errors=0):
        today = datetime.datetime.now().date().isoformat()
        try:
            self.conn.execute('INSERT INTO bot_stats (date, checked, answered, errors) VALUES (?, ?, ?, ?) ON CONFLICT(date) DO UPDATE SET checked=checked+excluded.checked, answered=answered+excluded.answered, errors=errors+excluded.errors',(today,checked,answered,errors))
            self.conn.commit()
        except Exception:
            try:
                cur = self.conn.execute('UPDATE bot_stats SET checked=checked+?, answered=answered+?, errors=errors+? WHERE date=?', (checked,answered,errors,today))
                if cur.rowcount == 0: self.conn.execute('INSERT INTO bot_stats (date,checked,answered,errors) VALUES (?,?,?,?)',(today,checked,answered,errors))
                self.conn.commit()
            except Exception as e: logging.error(f"Stats update fallback failed: {e}")

    def close(self):
        if self.conn:
            try: self.conn.close()
            except: pass


class EmailBotResponder:
    """Generates basic HTML responses as a fallback"""
    @staticmethod
    def create_price_card(data):
        if not data: return ""
        try:
            color = '#16a34a' if data.get('daily_change', 0) >= 0 else '#dc2626'
            return f"""<div style="display:inline-block;width:45%;margin:10px 2%;padding:15px;border-radius:12px;background:#f0f7ff;vertical-align:top;"><h3 style="margin:0;color:#1e40af;text-transform:uppercase;">{data.get('name','?')}</h3><p style="font-size:28px;font-weight:bold;margin:8px 0;">${data.get('price',0):,.2f}</p><p style="color:{color};font-size:16px;">{data.get('daily_change',0):+.2f}% today</p></div>"""
        except: return ""

    @staticmethod
    def generate_html_response(question, market_data):
        if not market_data: return EmailBotResponder.generate_help_response(question)
        cards = [EmailBotResponder.create_price_card(d) for d in market_data.values()]
        return f"""<!DOCTYPE html><html><head><meta charset="UTF-8"><style>body{{font-family:sans-serif;}}</style></head><body><h1>Basic Report</h1><p><b>Q:</b> {question}</p>{''.join(cards)}<p><i>Intelligent analysis failed.</i></p></body></html>"""
    
    @staticmethod
    def generate_help_response(question):
        return f"""<!DOCTYPE html><html><body><h1>Help</h1><p>No market topics detected in "{question}". Try asking about specific stocks (AAPL) or crypto (Bitcoin/XRP).</p></body></html>"""


class MarketQuestionAnalyzer:
    """Extracts topics and fetches data for user questions"""
    @staticmethod
    def extract_topics(question):
        topics, seen = {}, set()
        if not question: return topics
        q_lower = question.lower()
        mappings = {'bitcoin':{'t':'BTC-USD','n':'Bitcoin'},'btc':{'t':'BTC-USD','n':'Bitcoin'},'ethereum':{'t':'ETH-USD','n':'Ethereum'},'eth':{'t':'ETH-USD','n':'Ethereum'},'xrp':{'t':'XRP-USD','n':'Ripple'},'ripple':{'t':'XRP-USD','n':'Ripple'},'gold':{'t':'GC=F','n':'Gold'},'silver':{'t':'SI=F','n':'Silver'},'oil':{'t':'CL=F','n':'Crude Oil'},'sp500':{'t':'^GSPC','n':'S&P 500'},'nasdaq':{'t':'^IXIC','n':'Nasdaq'}}
        for keyword, data in mappings.items():
            if keyword in q_lower and data['t'] not in seen:
                asset_type = 'crypto' if '-USD' in data['t'] else 'commodity' if '=F' in data['t'] else 'index'
                topics[keyword] = {'type':asset_type, 'ticker':data['t'], 'name':data['n']}
                seen.add(data['t'])
        try:
            for ticker in re.findall(r'\$?([A-Z]{1,5})\b', question):
                if ticker not in ['USD','ETH','BTC','CEO','AI'] and len(ticker)<=5 and ticker not in seen:
                    topics[ticker.lower()] = {'type':'stock', 'ticker':ticker, 'name':ticker}
                    seen.add(ticker)
        except: pass
        return topics
    
    @staticmethod
    async def get_market_data(ticker):
        try:
            hist = await asyncio.to_thread(yf.Ticker(ticker).history, period='3mo')
            if hist.empty: return None
            current, day_ago = hist['Close'].iloc[-1], hist['Close'].iloc[-2] if len(hist)>1 else hist['Close'].iloc[-1]
            try: rsi_val = RSIIndicator(hist['Close'],14).rsi().iloc[-1]; rsi = 50.0 if pd.isna(rsi_val) else float(rsi_val)
            except: rsi = 50.0
            return {'ticker':ticker,'price':float(current),'daily_change':float(((current-day_ago)/day_ago*100) if day_ago else 0),'rsi':rsi}
        except Exception as e: logging.warning(f"Data fetch failed for {ticker}: {e}"); return None

# ========================================
# FINAL, AUDITED BOT SYSTEM - v4.2
# ========================================

class IntelligentMarketAnalyzer:
    def __init__(self):
        self.nlp = None
        self.wiki = None
        self.llm_clients = {}
        if 'SPACY_AVAILABLE' in globals() and SPACY_AVAILABLE:
            try: self.nlp = spacy.load("en_core_web_sm")
            except: logging.warning("spaCy model not loaded")
        if 'WIKIPEDIA_AVAILABLE' in globals() and WIKIPEDIA_AVAILABLE:
            self.wiki = wikipediaapi.Wikipedia('MarketBot/1.0','en')
        self._setup_llm_clients()

    def _setup_llm_clients(self):
        # CORRECTED: try/except blocks are now properly structured.
        if 'GROQ_AVAILABLE' in globals() and GROQ_AVAILABLE and os.getenv("GROQ_API_KEY"):
            try:
                from groq import Groq
                self.llm_clients['groq'] = Groq(api_key=os.getenv("GROQ_API_KEY"))
                logging.info("‚úÖ Groq LLM available")
            except Exception as e:
                logging.warning(f"Groq setup failed: {e}")
        if 'COHERE_AVAILABLE' in globals() and COHERE_AVAILABLE and os.getenv("COHERE_API_KEY"):
            try:
                import cohere
                self.llm_clients['cohere'] = cohere.Client(os.getenv("COHERE_API_KEY"))
                logging.info("‚úÖ Cohere LLM available")
            except Exception as e:
                logging.warning(f"Cohere setup failed: {e}")

    async def answer_intelligently(self, question, ticker_data):
        logging.info(f"üß† Generating intelligent answer for: {ticker_data.get('name', 'asset')}")
        intent = self._analyze_question_intent(question)
        context = await self._gather_context(ticker_data.get('name', ''), intent)
        response = await self._generate_llm_response(question, context, ticker_data)
        if not response or len(response) < 50:
            response = self._intelligent_assembly(context, ticker_data)
        return response
    
    def _analyze_question_intent(self, question):
        q = (question or "").lower()
        return {'reasons': any(w in q for w in ['why','reason']),'applications': any(w in q for w in ['application','use'])}

    async def _gather_context(self, asset_name, intent):
        context = {'news': [], 'wikipedia': {}}
        async def fetch_news():
            if DDGS_AVAILABLE and asset_name:
                try:
                    with DDGS() as ddgs:
                        query = f"{asset_name} analysis"
                        if intent.get('reasons'): query = f"{asset_name} price drivers"
                        elif intent.get('applications'): query = f"{asset_name} use cases"
                        news = list(ddgs.news(query,max_results=3))
                        context['news'] = [{'title': n.get('title',''),'body':n.get('body','')} for n in news]
                except Exception as e: logging.warning(f"News search failed: {e}")
        async def fetch_wiki():
            if self.wiki and asset_name:
                try:
                    page = self.wiki.page(asset_name)
                    if page.exists(): context['wikipedia']['summary'] = page.summary[:1000]
                except Exception as e: logging.warning(f"Wikipedia failed: {e}")
        await asyncio.gather(fetch_news(), fetch_wiki())
        return context
    
    async def _generate_llm_response(self, question, context, ticker_data):
        prompt = self._build_llm_prompt(question, context, ticker_data)
        response = None
        if 'groq' in self.llm_clients:
            try:
                client = self.llm_clients['groq']
                completion = client.chat.completions.create(model="llama-3.3-70b-versatile",messages=[{"role":"user","content":prompt}],temperature=0.7,max_tokens=800)
                response = completion.choices[0].message.content
                if response: logging.info("‚úÖ Groq response generated")
            except Exception as e: logging.warning(f"Groq failed: {e}")
        if not response and 'cohere' in self.llm_clients:
            try:
                client = self.llm_clients['cohere']
                result = client.chat(message=prompt,model='command-a-03-2025',temperature=0.7)
                response = result.text
                if response: logging.info("‚úÖ Cohere response generated")
            except Exception as e: logging.warning(f"Cohere failed: {e}")
        return response
    
    def _build_llm_prompt(self, question, context, ticker_data):
        parts = [f"Q: {question}",f"Asset: {ticker_data.get('name')}",f"Data: Price ${ticker_data.get('price',0):.2f}, RSI {ticker_data.get('rsi',50):.0f}"]
        if context.get('news'): parts.append("News:\n"+"\n".join([f"- {n['title']}" for n in context['news']]))
        if context.get('wikipedia',{}).get('summary'): parts.append(f"Background:\n{context['wikipedia']['summary']}")
        parts.append("As a market analyst, provide a direct, professional analysis.")
        return "\n".join(parts)

    def _intelligent_assembly(self, context, ticker_data):
        lines = [f"# {ticker_data.get('name')} Analysis",f"Price: ${ticker_data.get('price',0):,.2f}, RSI: {ticker_data.get('rsi',50):.0f}"]
        if context.get('news'): lines.extend(["## Key Drivers (from news)"]+[f"- {n['title']}" for n in context['news']])
        if context.get('wikipedia',{}).get('summary'): lines.extend([f"## Overview", context['wikipedia']['summary']])
        return '\n'.join(lines)


class IntelligentEmailBotResponder:
    def __init__(self):
        self.analyzer = IntelligentMarketAnalyzer()
    
    async def generate_intelligent_html(self, question, market_data):
        analyses = {}
        for key, data in market_data.items():
            if data:
                try: analyses[key] = {'data':data,'analysis':await self.analyzer.answer_intelligently(question,data)}
                except Exception as e: logging.error(f"Analysis failed for {key}: {e}")
        return self._build_intelligent_html(question, analyses)
    
    def _build_intelligent_html(self, question, analyses):
        def md_to_html(text):
            if 'MARKDOWN_AVAILABLE' in globals() and MARKDOWN_AVAILABLE:
                import markdown
                return markdown.markdown(text, extensions=['fenced_code','tables'])
            return f"<pre>{text}</pre>"
        
        cards = [EmailBotResponder.create_price_card(item['data']) for item in analyses.values()]
        sections = [f'<div style="margin-top:20px;padding:20px;background:#f9fafb;border-radius:10px;">{md_to_html(item["analysis"])}</div>' for item in analyses.values()]
        
        return f"""<!DOCTYPE html><html><head><meta charset="UTF-8"><style>body{{font-family:-apple-system,sans-serif;max-width:800px;margin:auto;}}</style></head><body><h1>Intelligent Analysis</h1><div><b>Q:</b> {question}</div><div>{''.join(cards)}</div>{''.join(sections)}</body></html>"""

class EmailBotEngine:
    """SIMPLE VERSION - Just processes YOUR emails only"""
    
    def __init__(self):
        self.db = None
        self.smtp_user = os.getenv("SMTP_USER")
        self.smtp_pass = os.getenv("SMTP_PASS")
        
        # YOUR EMAIL ADDRESS - hardcode it here
        self.authorized_email = "puneetbr44@gmail.com"
        
        if not self.smtp_user or not self.smtp_pass:
            raise ValueError("‚ùå SMTP credentials required!")
        
        logging.info(f"üìß Bot initialized. Will only respond to: {self.authorized_email}")
        
        try:
            self.db = EmailBotDatabase()
        except Exception as e:
            logging.error(f"DB init failed: {e}")

    async def check_and_respond(self):
        """SIMPLIFIED: Only process emails from YOUR address"""
        logging.info(f"üìß Checking inbox for emails from {self.authorized_email}...")
        checked, answered, errors = 0, 0, 0
        mail = None
        
        try:
            # 1. CONNECT
            mail = imaplib.IMAP4_SSL("imap.gmail.com", timeout=20)
            mail.login(self.smtp_user, self.smtp_pass)
            mail.select('inbox')
            
            # 2. SIMPLE SEARCH: Get UNSEEN emails from YOUR address only
            search_query = f'(UNSEEN FROM "{self.authorized_email}")'
            status, data = mail.search(None, search_query)
            
            if status != 'OK' or not data[0]:
                logging.info(f"‚úÖ No new emails from {self.authorized_email}")
                return
            
            email_ids = data[0].split()
            logging.info(f"üì¨ Found {len(email_ids)} new email(s) from you")

            # 3. PROCESS ONLY THE MOST RECENT 5
            for eid in sorted(email_ids, key=int, reverse=True)[:5]:
                try:
                    checked += 1
                    _, fdata = mail.fetch(eid, '(RFC822)')
                    msg = email.message_from_bytes(fdata[0][1])
                    
                    question = self._extract_question(msg)
                    subject = msg.get('Subject', 'Market Question')
                    
                    logging.info(f"‚ùì Question: {question[:100]}")
                    
                    if not self._is_valid(question):
                        logging.warning(f"‚è≠Ô∏è Skipping invalid/empty question")
                        mail.store(eid, '+FLAGS', '\\Seen')
                        continue
                    
                    # Extract topics and get market data
                    topics = MarketQuestionAnalyzer.extract_topics(question)
                    
                    if not topics:
                        html = EmailBotResponder.generate_help_response(question)
                    else:
                        tickers = [info['ticker'] for info in topics.values()]
                        results = await asyncio.gather(
                            *(MarketQuestionAnalyzer.get_market_data(t) for t in tickers)
                        )
                        final_market_data = {
                            key: {**info, **data} 
                            for (key, info), data in zip(topics.items(), results) 
                            if data
                        }
                        
                        # Try intelligent response
                        try:
                            responder = IntelligentEmailBotResponder()
                            html = await responder.generate_intelligent_html(question, final_market_data)
                        except Exception as e:
                            logging.warning(f"Intelligent responder failed: {e}. Using basic.")
                            html = EmailBotResponder.generate_html_response(question, final_market_data)
                    
                    # Send response
                    if self._send_email(self.authorized_email, question, html, subject):
                        answered += 1
                        logging.info(f"‚úÖ Response sent!")
                        if self.db:
                            self.db.log_conversation(self.authorized_email, question, topics, True)
                    else:
                        errors += 1
                    
                    mail.store(eid, '+FLAGS', '\\Seen')
                    await asyncio.sleep(2)
                
                except Exception as e:
                    errors += 1
                    logging.error(f"Error processing email {eid.decode()}: {e}")

            logging.info(f"‚úÖ Complete: {answered}/{checked} answered, {errors} errors")

        except Exception as e:
            errors += 1
            logging.error(f"‚ùå Bot error: {e}", exc_info=True)
        
        finally:
            if mail:
                try:
                    mail.close()
                    mail.logout()
                except:
                    pass
            if self.db:
                self.db.update_stats(checked, answered, errors)

    def _send_email(self, to_email, question, html_body, original_subject=""):
        """Send email response"""
        try:
            msg = MIMEMultipart('alternative')
            subject = f"Re: {original_subject}" if original_subject else "Market Analysis"
            msg['Subject'] = subject
            msg['From'] = self.smtp_user
            msg['To'] = to_email
            msg['Date'] = email.utils.formatdate(localtime=True)
            
            msg.attach(MIMEText(f"Q: {question}\n\nSee HTML for analysis.", 'plain'))
            msg.attach(MIMEText(html_body, 'html'))
            
            with smtplib.SMTP("smtp.gmail.com", 587, 30) as s:
                s.starttls()
                s.login(self.smtp_user, self.smtp_pass)
                s.send_message(msg)
            
            return True
        except Exception as e:
            logging.error(f"Send failed: {e}")
            return False

    def _extract_question(self, msg):
        """Extract question text from email"""
        body = ""
        try:
            if msg.is_multipart():
                for part in msg.walk():
                    if part.get_content_type() == "text/plain":
                        try:
                            payload = part.get_payload(decode=True)
                            if payload:
                                body = payload.decode('utf-8', 'ignore')
                                break
                        except:
                            continue
            else:
                payload = msg.get_payload(decode=True)
                if payload:
                    body = payload.decode('utf-8', 'ignore')
            
            # Clean reply markers
            lines = [
                l.strip() 
                for l in (body or "").split('\n') 
                if l.strip() 
                and not l.strip().startswith('>') 
                and 'wrote:' not in l.lower()
            ]
            return re.sub(r'\s+', ' ', ' '.join(lines))[:1000]
        except:
            return ""

    def _is_valid(self, question):
        """Check if question is valid"""
        return bool(
            question 
            and len(question.strip()) > 5 
            and not any(k in question.lower() for k in ['automated', 'auto-reply', 'unsubscribe'])
        )


async def run_email_bot():
    """Entry point for email bot mode"""
    try:
        verify_intelligence_available()
        bot = EmailBotEngine()
        await bot.check_and_respond()
    except Exception as e:
        logging.error(f"‚ùå Bot initialization failed: {e}", exc_info=True)
        sys.exit(1)

def verify_intelligence_available():
    """Logs the status of intelligent components."""
    logging.info("üîç Verifying Intelligence Components Status:")
    components = {
        'spaCy': 'SPACY_AVAILABLE' in globals() and SPACY_AVAILABLE,
        'Wikipedia': 'WIKIPEDIA_AVAILABLE' in globals() and WIKIPEDIA_AVAILABLE,
        'Groq': 'GROQ_AVAILABLE' in globals() and GROQ_AVAILABLE and os.getenv("GROQ_API_KEY"),
        'Cohere': 'COHERE_AVAILABLE' in globals() and COHERE_AVAILABLE and os.getenv("COHERE_API_KEY"),
    }
    for component, available in components.items():
        logging.info(f"  {'‚úÖ' if available else '‚ùå'} {component}: {'Available' if available else 'Not Available'}")

# ========================================
# üÜï END EMAIL BOT SYSTEM
# ========================================

# ============================================================================
# üÜï ENHANCED PATTERN DETECTION (NEW - SEPARATE BLOCK)
# ============================================================================
# This runs AFTER the existing analysis and adds advanced pattern detection
# without modifying any of the working code above.
# ============================================================================

try:
    # Try multiple import paths (works in both local and GitHub Actions)
    try:
        from src.analysis.enhanced_patterns import EnhancedPatternDetector
    except ImportError:
        from analysis.enhanced_patterns import EnhancedPatternDetector
    
    ENHANCED_PATTERNS_ENABLED = True
    logging.info("‚úÖ Enhanced Pattern Detection loaded successfully")
    
    # üÜï Initialize it immediately (ADDED THIS)
    enhanced_pattern_detector = EnhancedPatternDetector()
    logging.info("‚úÖ Enhanced Pattern Detector initialized (30+ patterns)")
    
except ImportError as e:
    ENHANCED_PATTERNS_ENABLED = False
    enhanced_pattern_detector = None  # üÜï Set to None if import fails (ADDED THIS)
    logging.warning(f"‚ö†Ô∏è Enhanced Pattern Detection not available: {e}")
    logging.warning("Continuing without enhanced pattern detection...")



def add_enhanced_pattern_analysis(ticker_symbol: str, existing_analysis: dict, hist_data: pd.DataFrame) -> dict:
    """
    Adds enhanced pattern detection to existing analysis WITHOUT modifying it.
    This is a completely separate analysis layer.
    
    Args:
        ticker_symbol: Stock ticker
        existing_analysis: Your current analysis dict (unchanged)
        hist_data: Historical price data
        
    Returns:
        Enhanced analysis with additional pattern data
    """
    if not ENHANCED_PATTERNS_ENABLED:
        return existing_analysis
    
    try:
        # Create pattern detector
        detector = EnhancedPatternDetector()
        
        # Detect all patterns
        all_patterns = detector.detect_all_patterns(hist_data)
        strongest_pattern = detector.get_strongest_pattern(hist_data)
        
        # Add to analysis as a NEW section (doesn't touch existing data)
        enhanced = existing_analysis.copy()
        enhanced['enhanced_patterns'] = {
            'all_patterns_found': all_patterns,
            'strongest_pattern': strongest_pattern,
            'pattern_count': len(all_patterns),
            'top_3_patterns': all_patterns[:3] if len(all_patterns) >= 3 else all_patterns
        }
        
        # Log findings
        if strongest_pattern:
            logging.info(f"üïØÔ∏è {ticker_symbol} - Strongest pattern: {strongest_pattern['name']} "
                        f"({strongest_pattern['signal']}, strength: {strongest_pattern['strength']}%)")
        
        return enhanced
        
    except Exception as e:
        logging.error(f"Error in enhanced pattern detection for {ticker_symbol}: {e}")
        return existing_analysis


def enhance_prediction_with_patterns(ticker: str, prediction_data: dict, hist_data: pd.DataFrame) -> dict:
    """
    Enhance the final prediction by considering advanced patterns.
    This creates a SUPPLEMENTARY prediction without changing the original.
    
    Args:
        ticker: Stock ticker
        prediction_data: Original prediction from your existing system
        hist_data: Historical price data
        
    Returns:
        Enhanced prediction with pattern-based insights
    """
    if not ENHANCED_PATTERNS_ENABLED:
        return prediction_data
    
    try:
        detector = EnhancedPatternDetector()
        strongest = detector.get_strongest_pattern(hist_data)
        
        if not strongest:
            return prediction_data
        
        # Create enhanced prediction (keeps original intact)
        enhanced = prediction_data.copy()
        
        # Add pattern confirmation/conflict analysis
        original_action = prediction_data.get('action', 'HOLD')
        pattern_signal = strongest['signal']
        
        # Check if pattern agrees with original prediction
        signals_agree = (original_action == pattern_signal)
        
        enhanced['pattern_analysis'] = {
            'pattern_detected': strongest['name'],
            'pattern_signal': pattern_signal,
            'pattern_strength': strongest['strength'],
            'agrees_with_prediction': signals_agree,
            'pattern_description': strongest['description']
        }
        
        # Boost confidence if both agree and pattern is strong
        if signals_agree and strongest['strength'] >= 80:
            original_confidence = prediction_data.get('confidence', 50)
            confidence_boost = min(10, (strongest['strength'] - 70) / 2)  # Max +10 boost
            enhanced['confidence'] = min(100, original_confidence + confidence_boost)
            enhanced['confidence_boost_reason'] = f"Strong {strongest['name']} pattern confirms prediction"
            logging.info(f"üìà {ticker} confidence boosted: {original_confidence}% ‚Üí {enhanced['confidence']}%")
        
        # Warning if they conflict
        elif not signals_agree and strongest['strength'] >= 75:
            enhanced['warning'] = f"‚ö†Ô∏è Pattern suggests {pattern_signal} but prediction is {original_action}"
            logging.warning(f"‚ö†Ô∏è {ticker}: Pattern conflict - {strongest['name']} suggests {pattern_signal}")
        
        return enhanced
        
    except Exception as e:
        logging.error(f"Error enhancing prediction for {ticker}: {e}")
        return prediction_data


# ============================================================================
# üîß WRAPPER FUNCTION - Use this in your workflow
# ============================================================================

async def analyze_with_enhanced_patterns(ticker: str) -> dict:
    """
    Complete analysis with enhanced patterns.
    This wraps your existing analysis and adds pattern detection.
    
    Usage: Replace your current analyze_stock() call with this function
    """
    # Get historical data
    stock = yf.Ticker(ticker)
    hist = stock.history(period='3mo')
    
    if hist.empty:
        logging.warning(f"No data for {ticker}")
        return None
    
    # Run your EXISTING analysis (whatever you currently have)
    # This should call your current analysis functions
    # For now, I'll show the structure - you'll plug in your actual analysis
    
    # YOUR EXISTING ANALYSIS CODE HERE (example):
    # existing_analysis = await your_current_analysis_function(ticker)
    # For demonstration:
    existing_analysis = {
        'ticker': ticker,
        'action': 'HOLD',  # Your current prediction
        'confidence': 70,
        'reasoning': 'Your current reasoning',
        # ... all your existing fields
    }
    
    # ADD: Enhanced pattern analysis (NEW - doesn't touch above)
    enhanced_analysis = add_enhanced_pattern_analysis(ticker, existing_analysis, hist)
    
    # ADD: Pattern-based prediction enhancement (NEW)
    final_prediction = enhance_prediction_with_patterns(ticker, enhanced_analysis, hist)
    
    return final_prediction


# ============================================================================
# üìä REPORTING FUNCTION - Shows pattern insights
# ============================================================================

def print_pattern_summary(analysis_results: list):
    """
    Pretty print pattern detection summary.
    Call this after your analysis is complete.
    """
    if not ENHANCED_PATTERNS_ENABLED:
        return
    
    print("\n" + "="*80)
    print("üïØÔ∏è ENHANCED PATTERN DETECTION SUMMARY")
    print("="*80)
    
    for result in analysis_results:
        if not result or 'enhanced_patterns' not in result:
            continue
        
        ticker = result['ticker']
        patterns = result['enhanced_patterns']
        strongest = patterns.get('strongest_pattern')
        
        if strongest:
            emoji = "üü¢" if strongest['signal'] == 'BUY' else "üî¥" if strongest['signal'] == 'SELL' else "‚ö™"
            print(f"\n{emoji} {ticker}:")
            print(f"   Pattern: {strongest['name'].replace('_', ' ').title()}")
            print(f"   Signal: {strongest['signal']} (Strength: {strongest['strength']}%)")
            print(f"   Type: {strongest['type'].replace('_', ' ').title()}")
            
            # Show if it agrees with main prediction
            if 'pattern_analysis' in result:
                pa = result['pattern_analysis']
                if pa['agrees_with_prediction']:
                    print(f"   ‚úÖ Confirms main prediction")
                else:
                    print(f"   ‚ö†Ô∏è Conflicts with main prediction")
            
            # Show top 3 patterns if multiple found
            if patterns['pattern_count'] > 1:
                print(f"   üìä Found {patterns['pattern_count']} total patterns")
    
    print("\n" + "="*80 + "\n")


# ============================================================================
# üéØ EXAMPLE INTEGRATION WITH YOUR EXISTING main()
# ============================================================================

# Add this to your existing main() function or create a new one:

async def enhanced_main():
    """
    Example of how to use enhanced patterns with your existing code.
    This shows the integration pattern - adapt to your actual code.
    """
    
    tickers = ['AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMD']  # Your tickers
    results = []
    
    for ticker in tickers:
        try:
            # ============================================================
            # YOUR EXISTING ANALYSIS - KEEP AS IS
            # ============================================================
            # result = await your_existing_analysis_function(ticker)
            
            # For demonstration, simulating your existing analysis:
            stock = yf.Ticker(ticker)
            hist = stock.history(period='3mo')
            
            # Your current prediction logic (unchanged)
            existing_result = {
                'ticker': ticker,
                'action': 'HOLD',  # From your LLM
                'confidence': 75,   # From your system
                'reasoning': 'Your existing reasoning'
            }
            
            # ============================================================
            # NEW: ADD ENHANCED PATTERNS (SEPARATE LAYER)
            # ============================================================
            enhanced_result = add_enhanced_pattern_analysis(
                ticker, 
                existing_result, 
                hist
            )
            
            final_result = enhance_prediction_with_patterns(
                ticker,
                enhanced_result,
                hist
            )
            
            results.append(final_result)
            
        except Exception as e:
            logging.error(f"Error analyzing {ticker}: {e}")
    
    # Print summary
    print_pattern_summary(results)
    
    return results


# ============================================================================
# ADD THIS: Enhanced Pattern Detection Integration
# ============================================================================

def add_patterns_to_prediction(ticker: str, prediction_data: dict) -> dict:
    """
    Add enhanced pattern analysis to existing prediction.
    Call this after creating a prediction but before storing it.
    """
    if not ENHANCED_PATTERNS_ENABLED:
        return prediction_data
    
    try:
        # Get historical data
        stock = yf.Ticker(ticker)
        hist = stock.history(period='3mo')
        
        if hist.empty or len(hist) < 10:
            return prediction_data
        
        # Detect patterns
        detector = EnhancedPatternDetector()
        strongest = detector.get_strongest_pattern(hist)
        all_patterns = detector.detect_all_patterns(hist)
        
        if strongest:
            # Add pattern info to prediction
            prediction_data['strongest_pattern'] = strongest['name']
            prediction_data['pattern_signal'] = strongest['signal']
            prediction_data['pattern_strength'] = strongest['strength']
            prediction_data['pattern_count'] = len(all_patterns)
            
            # Log it
            logging.info(f"üïØÔ∏è {ticker} pattern: {strongest['name']} "
                        f"({strongest['signal']}, {strongest['strength']}%)")
            
            # Boost confidence if pattern confirms prediction
            if strongest['signal'] == prediction_data.get('action') and strongest['strength'] >= 80:
                old_conf = prediction_data.get('confidence', 50)
                boost = min(10, (strongest['strength'] - 70) / 2)
                prediction_data['confidence'] = min(100, old_conf + boost)
                logging.info(f"   üìà Confidence boosted: {old_conf}% ‚Üí {prediction_data['confidence']}%")
            
            # Warn if pattern conflicts
            elif strongest['signal'] != prediction_data.get('action') and strongest['strength'] >= 75:
                logging.warning(f"   ‚ö†Ô∏è Pattern conflict: {strongest['name']} suggests "
                              f"{strongest['signal']} but prediction is {prediction_data.get('action')}")
        
        return prediction_data
        
    except Exception as e:
        logging.error(f"Error adding patterns to {ticker}: {e}")
        return prediction_data

# ============================================================================
# üß™ PATTERN DETECTION TEST - Add before if __name__ == "__main__"
# ============================================================================

async def test_pattern_detection_quick():
    """Quick test to verify pattern detection works"""
    if not ENHANCED_PATTERNS_ENABLED:
        logging.info("‚ö†Ô∏è Pattern detection not enabled")
        return
    
    logging.info("\n" + "="*80)
    logging.info("üß™ TESTING PATTERN DETECTION")
    logging.info("="*80)
    
    test_tickers = ['AAPL', 'NVDA', 'TSLA']
    
    for ticker in test_tickers:
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period='3mo')
            
            if hist.empty or len(hist) < 10:
                logging.info(f"‚ùå {ticker}: Not enough data")
                continue
            
            detector = EnhancedPatternDetector()
            strongest = detector.get_strongest_pattern(hist)
            all_patterns = detector.detect_all_patterns(hist)
            
            if strongest:
                emoji = "üü¢" if strongest['signal'] == 'BUY' else "üî¥" if strongest['signal'] == 'SELL' else "‚ö™"
                logging.info(f"{emoji} {ticker}: {strongest['name']} "
                           f"({strongest['signal']}, {strongest['strength']}%) "
                           f"- Found {len(all_patterns)} patterns total")
            else:
                logging.info(f"‚ö™ {ticker}: No clear patterns detected")
                
        except Exception as e:
            logging.error(f"‚ùå {ticker}: Error - {e}")
            import traceback
            traceback.print_exc()
    
    logging.info("="*80 + "\n")


# ========================================
# PROGRAM ENTRY POINT
# ========================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Market Intelligence System v2.1.0"
    )
    parser.add_argument(
        "--output", 
        default="print", 
        choices=["print", "email"], 
        help="Where to send analysis: 'print' to console or 'email' to inbox"
    )
    parser.add_argument(
        "--check-emails",
        action="store_true",
        help="Run in email bot mode (check inbox and respond to questions)"
    )
    
    args = parser.parse_args()
    
    # Windows event loop compatibility
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    logging.info("=" * 60)
    
    if args.check_emails:
        logging.info("ü§ñ EMAIL BOT MODE - Market Q&A System")
        logging.info("=" * 60)
        asyncio.run(run_email_bot())
    else:
        logging.info("üöÄ MARKET INTELLIGENCE SYSTEM v2.1.0")
        logging.info("=" * 60)
        asyncio.run(main(output=args.output))
# ============================================================================
# üÜï NEWS/EVENTS ENGINE INITIALIZATION (NEW - SEPARATE BLOCK)
# ============================================================================
try:
    from analysis.news_events_engine import NewsEventsEngine
    news_events_engine = NewsEventsEngine()
    NEWS_EVENTS_ENABLED = True
    logging.info("‚úÖ News/Events Engine loaded successfully")
except ImportError as e:
    news_events_engine = None
    NEWS_EVENTS_ENABLED = False
    logging.warning(f"‚ö†Ô∏è News/Events Engine not available: {e}")
# ============================================================================
